\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_09,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\usepackage{multirow}
\usepackage{tabularx}
%\usepackage{adjustbox}


\title{CSE221}

\author{
  Boyuan Qin\\
  \texttt{bqin@eng.ucsd.edu}\\
  \And
  Denxi Qi\\
  \texttt{deqi@eng.ucsd.edu}\\
  \And
  Qiheng Wang\\
  \texttt{qiw018@eng.ucsd.edu}\\
}
\date{\today}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\section{Introduction}
The goal of this project is to measure the performance of the operating system, Ubuntu 12.04.4 LTS. The reason behind this goal is to gain an understanding of the relationship between the underlying hardware and the operating system, and their effects on how much time integral operations will require. With this goal, we will be running a series of justified experiments that will allow us to characterize the speed of each operation and compare them to each other. We will use the course of these experiments as a way of gauging our intuition about the performance of the operating system we are testing. The information we will obtain from these experiments will also be used in future endeavors as a set of performance results to compare against. 

The language of choice in these tests will be C++. The reason for choosing C++ comes from the flexibility that C++ provides. It is a high-level language that allows us to, at the same time, work without much overhead as, for example, Java when measuring time. The compiler we are using is 4.6.3-1ubuntu5 with no optimization.

The first part of the project required around 20 hours of work to complete. This included creating the tools necessary to measure the operating system and writing the report. 

The experiments were split accordingly:

Measurement Overhead - Boyuan

Procedure Call Overhead - Denxi

System Call Overhead - Boyuan/Qiheng

Task Creation Time - Denxi

Context Switch Time

\section{Machine Description}
\begin{table}[h]
  \caption{Machine Specifications}
  \begin{center}
	\resizebox{\columnwidth}{!}{
  \begin{tabular}{|>{\centering\arraybackslash\bfseries}m{1in}|l|l|}
	\hline
	\multirow{8}{*}{Processor} & Model                    & Intel\textregistered Core\texttrademark 2 Duo Processor P8600  \\ \cline{2-3}
	& Instruction Set          & 64 bit                                                         \\ \cline{2-3}
	& Cycle Time               & 0.417ns(2.4GHz frequency)                                      \\ \cline{2-3}
	& L1 data cache            & 32KB per core, 8-way set associative, 64-byte line size        \\ \cline{2-3}
	& L1 instruction cache     & 32KB per, core 8-way set associative, 64-byte line size        \\ \cline{2-3}
	& L2 data cache            & 3072KB, 12-way set associative, 64-byte line size              \\ \cline{2-3}
	& FSB                      & 1066 MHz                                                       \\ 
	\hline
	\multirow{11}{*}{Hard Drive}        & Model                    & Seagate Momentus\textregistered  5400.6 SATA model ST9500325AS \\ \cline{2-3}
	& Capacity                 & 500GB                                                          \\ \cline{2-3}
	& Cache                    & 8 Mbytes                                                       \\ \cline{2-3}
	& RPM                      & 5400                                                           \\ \cline{2-3}
	& Physical heads           & 4                                                              \\ \cline{2-3}
	& Discs                    & 2                                                              \\ \cline{2-3}
	& Average Seek Read        & 14ms typical                                                   \\ \cline{2-3}
	& Full Stroke Seek         & 30ms                                                           \\ \cline{2-3}
	& Average Latency          & 5.6ms                                                          \\ \cline{2-3}
	& Track to track seek time & 1ms typical                                                    \\ \cline{2-3}
	& I/O data transfer rate   & 300 Mbytes/s max                                               \\ 
	\hline
	\multirow{3}{*}{Memory}             & Capacity                 & Dual channel(symetric), each 1024 MBytes                       \\ \cline{2-3}
	& Frequency                & DDR3 PC3-10700 667 MHz                                         \\ \cline{2-3}
	& Width                    & 64 bit per channel                                             \\ 
	\hline
	\multirow{2}{*}{Network Card}       & Model                    & Intel Wifi Link 5100                                           \\ \cline{2-3}
	& Data Transfer Rate       & 300Mbps                                                        \\ 
	\hline
	\multicolumn{1}{|>{\bfseries}c|}{\multirow{2}{*}{OS}}   & \multirow{2}{*}{ Linux Distribution }       & Ubuntu 12.04.4 LTS            \\
	& \multicolumn{1}{c|}{}        &  GNU/Linux 3.8.0-35-generic i686 \\
	% here if use |c| instead of c|, will generate an extra vertical line
	\hline
  \end{tabular}
} %end of resizebox
  \end{center}
  \label{table:machine_description}
\end{table}


\section{CPU, Scheduling, and OS Services}

\subsection{Measurement Overhead}
The goal of this section is to report the overhead of reading time and the overhead of using a loop to measure many iterations of an operation. These measurements are important and necessary for accuracy of future experiments. 

\subsubsection{Experiment Methodology}
To measure the overhead time for reading time, we measured the time of the getticks() function and elapsed() from cycle.h. In order to get the time needed to execute a single getticks() method, we simply executed getticks() twice in a row and took the difference. The idea behind this methodology was to start tracking immediately before the the execution of the next command and stop tracking as soon as the command is executed. The difference between the two getticks() times will give us the number of ticks taken between the two methods. Similarly, measuring elapsed() required that we add two getticks(), one before and one after, the elapsed() method. We then determined the difference between the two getticks() methods that preceded and followed the elapsed() method. Taking that difference, we subtracted the time taken for one getticks() method to account for the time necessary to execute the succeeding getticks() to arrive at our final result. 
  
To measure the overhead time for using a loop, we chose to measure a for() loop that will increment a variable, x, from 0 to 10000, and divided it by 10000 to obtain the time taken by the for() loop for a single increment and check operation. In order to measure purely the looping part of the for() loop, we put the variable declaration before the first getticks() method. This way, we measured the time taken by the loop to check and increment the variable. We also took into account the time taken by the getticks() method and subtracted it from our final time. Our motive behind choosing to test the for() loop comes from our own foresight of using for() loops plentifully in future tests.

Each test was ran 10000 times and the average was taken for the result.

In order to accurately translate 'ticks' into human time, we A preliminary measurement of the correlation between ticks and number of cycles is 1 to 1. Since each cycle takes 0.416 ns, we can make the correlation that 1 tick is equivalent to 0.416 ns.
\subsubsection{Prediction}

\subsubsection{Experiment Results}

\begin{table}
  \caption{Reading overhead (in cycles)}
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation     & Hardware Est. & Software Est. & Predicted  & Measured     & Standard Deviation \\ \hline
    getticks()    & 4 (1.6 ns)    & 2 (0.8 ns)    & 6 (2.4 ns) & 45 (18.5 ns) & ~                  \\ \hline
    elapsed()     & 6 (2.4 ns)    & 2 (0.8 ns)    & 8 (3.2 ns) & 36 (14.4 ns) & ~                  \\ \hline
    for() loop    & 3 (1.2 ns)    & 2 (0.8 ns)    & 5 (2.0 ns) & 7 (2.8 ns)   & ~                  \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Result Discussion}

The predicted performance was less than the measured performance. Our predicted performance was based on the naive assumption that one task, such as a jump, would only take 1 cycle to perform. However, looking at the measured operation times, our assumption was proven to be false. This means that the operating system and hardware were taking more cycles to perform each task. The other consideration that we did not make was the different operations needed to jump to the external file where the code for getticks() and elapsed() resided. In such a case and if our original assumption was correct, then the extra time taken that we did not account for was spent in the other such operations. On the other hand, the measured operation time was close to the predicted operation time for the for() loop. The difference here resided in our overlooking one or two operations while making our prediction

Our methodology centered around measuring the simplest possible operation for reading time and for looping. Because we measured time immediately before and immediately after the operation of interest, we successfully captured the entirety of the operation. With this information, we will be able to use it as a base from which we can accurately measure other performances.

\subsection{Procedure Call Overhead}
The goal of this section is to report the overhead of making a procedure call with arguments. The number of arguments the procedure takes in will also be taken into account in a series of tests. Each test will take in different amounts of arguments ranging from 0 arguments to 7 arguments. These measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a procedure call, we measured the time to make a call to a procedure that takes in the parameters given and does not processing thereafter. The procedure itself does not return anything either. In order to take into account the different time costs as a result of varying amounts of parameters, we made procedure calls with 0 to 7 parameters. Each procedure call with different amounts of parameters was tested 1000 and the average was taken as the result. Each procedure call was precluded and succeeded by two different getticks(). Because of this, we also took into account the amount of overhead getticks() populated and subtracted that from the total in each procedure call. 

Each of the procedure call did no processing of its arguments nor did it have a return statement. The decision to make the procedure call in this way was motivated by our goal of obtaining purely the overhead of making the call to a procedure. It was decided that return statements and any processing of data would not be part of the procedure call itself, and thus not included as part of the testing processes. 

\subsection{System Call Overhead}
The goal of this section is to report the overhead of making a system call and compare it to the cost of making a procedure call. The system call with the least overhead was chosen to accurately capture the time taken to make the the switch from user mode to kernel mode. These measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a system call, we attempted to choose a method with as little overhead as possible. Initially, our goal was to used getpid() as our system call of choice. However, due to the potential problem of having the getpid() system call getting cached and thus not having an accurate overhead time of trapping into the kernel space, we decided to choose a similar method, getppid(). In order to measure the time of getppid(), we went about by simply adding getticks() before and after the system call and calculated the average difference over multiple runs. This method was similar to the methodology described earlier in measuring loop overhead. We then compared the results that we received to the results we measured earlier for procedure calls.

\subsection{Task Creation Time}
The goal of this section is to report the time taken to create and run both a process and a kernel thread, and to compare the two time costs.  

\subsubsection{Experiment Methodology}
In order to measure time taken to create and run both a process and a kernel thread, we used the basic principle of precluding and succeeding the target section of code as we have prior. In order to create a kernel thread, we used the method kthread\_run() in kthread.h to create and run a thread. The inputs for kthread\_run() includes a function to run, a data pattern for the function, and a printf\-style name for the thread. In order to focus our measurements on creating and running the simplest kernel thread, we gave kthread\_run() an external function that only called do\_exit(), no data, and an 8 character name. Because we want the thread to stop as soon as it is created an runs, we called do\_exit() to stop the thread within the called function. This way, we can eliminate any extra overhead time from any extra processing done in the function. This test was repeated 10000 times and and average was taken while taking into consideration the overhead of reading time. We used the same procedures for processes. We created processes using fork() and terminated the process using \_exit().

One point of difference in our methodology for measuring time for kernel threads that differed from previous methodologies was the use of rdtsc() from rdtsc.h to calculate time. The reason for using rdtsc() in measuring kernel thread was the inability to use getticks() in kernel space. The difference in reading time for rdtsc() was measured before measurements for kernel thread were made, and was used when taking into account overhead time for reading time. 

\subsection{Context Switch Time}
The goal of this section is to report the time taken to context switch from one process to another in comparison to the time taken to context switch from one kernel thread to another. 

\end{document}
