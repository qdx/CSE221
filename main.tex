\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_09,times}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[T1]{fontenc}
%\usepackage{adjustbox}


\title{CSE221 Operating System Measurement}

\author{
  Boyuan Qin\\
  \texttt{bqin@eng.ucsd.edu}\\
  \And
  Dexin Qi\\
  \texttt{deqi@eng.ucsd.edu}\\
  \And
  Qiheng Wang\\
  \texttt{qiw018@eng.ucsd.edu}\\
}
\date{\today}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\section{Introduction}
The goal of this project is to measure the performance of the operating
system, Ubuntu 12.04.4 LTS\@.  The reason behind this goal is to gain an
understanding of the relationship between the underlying hardware and the
operating system, and their effects on how much time integral operations will
require.  With this goal, we will be running a series of justified experiments
that will allow us to characterize the speed of each operation and compare
them to each other.  We will use the course of these experiments as a way of
gauging our intuition about the performance of the operating system we are
testing.  The information we will obtain from these experiments will also be
used in future endeavors as a set of performance results to compare against.

The language of choice in these tests will be C++.  The reason for choosing
C++ comes from the flexibility that C++ provides.  It is a high-level language
that allows us to, at the same time, work without much overhead as, for
example, Java when measuring time.  The compiler we are using is GCC
4.6.3-1ubuntu5 with no optimization.

The first part of the project required around 20 hours of work to complete. This included creating the tools necessary to measure the operating system and writing the report.

The experiments were split accordingly:

Measurement Overhead - Boyuan

Procedure Call Overhead - Dexin

System Call Overhead - Boyuan/Qiheng

Task Creation Time - Dexin

Context Switch Time - Boyuan/Qiheng

%============================= section ==========================%
\section{Machine Description}

The machine we used as the subject of our experiments is described in Table 1.

\begin{table}[h]
  \caption{Machine Specifications}
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{|>{\centering\arraybackslash\bfseries}m{1in}|l|l|}
	\hline
	\multirow{8}{*}{Processor}          & Model                    & Intel\textregistered Core\texttrademark 2 Duo Processor P8600  \\ \cline{2-3}
	& Instruction Set                   & 64 bit                                                         \\ \cline{2-3}
	& Cycle Time                        & 0.417ns(2.4GHz frequency)                                      \\ \cline{2-3}
	& L1 data cache                     & 32KB per core, 8-way set associative, 64-byte line size        \\ \cline{2-3}
	& L1 instruction cache              & 32KB per, core 8-way set associative, 64-byte line size        \\ \cline{2-3}
	& L2 data cache                     & 3072KB, 12-way set associative, 64-byte line size              \\ \cline{2-3}
	& FSB                               & 1066 MHz                                                       \\
	\hline
	\multirow{11}{*}{Hard Drive}        & Model                    & Seagate Momentus\textregistered  5400.6 SATA model ST9500325AS \\ \cline{2-3}
	& Capacity                          & 500GB                                                          \\ \cline{2-3}
	& Cache                             & 8 Mbytes                                                       \\ \cline{2-3}
	& RPM                               & 5400                                                           \\ \cline{2-3}
	& Physical heads                    & 4                                                              \\ \cline{2-3}
	& Discs                             & 2                                                              \\ \cline{2-3}
	& Average Seek Read                 & 14ms typical                                                   \\ \cline{2-3}
	& Full Stroke Seek                  & 30ms                                                           \\ \cline{2-3}
	& Average Latency                   & 5.6ms                                                          \\ \cline{2-3}
	& Track to track seek time          & 1ms typical                                                    \\ \cline{2-3}
	& I/O data transfer rate            & 300 Mbytes/s max                                               \\
	\hline
	\multirow{3}{*}{Memory}             & Capacity                 & Dual channel(symetric), each 1024 MBytes                       \\ \cline{2-3}
	                                    & Frequency                & DDR3 PC3-10700 667 MHz                                         \\ \cline{2-3}
	                                    & Width                    & 64 bit per channel                                             \\
	\hline
	\multirow{2}{*}{Network Card}       & Model                    & Intel Wifi Link 5100                                           \\ \cline{2-3}
	                                    & Data Transfer Rate       & 300Mbps                                                        \\
	\hline
	\multicolumn{1}{|>{\bfseries}c|}{\multirow{2}{*}{OS}}   & \multirow{2}{*}{ Linux Distribution }       & Ubuntu 12.04.4 LTS               \\
	                                                        & \multicolumn{1}{c|}{}                       & GNU/Linux 3.8.0-35-generic i686 \\
	% here if use |c| instead of c|, will generate an extra vertical line
	\hline
  \end{tabular}
} % end of resizebox
  \end{center}
  \label{table:machine_description}
\end{table}

%-----------------------------------------------------------------------------
% Milestone 1: CPU, Scheduling, and OS Services
%-----------------------------------------------------------------------------
\section{CPU, Scheduling, and OS Services}

To accomplish following set of measurements, testing against a normally
running OS is insufficient. Therefore, we modified a few kernel settings so
that the test results are more accurate and closer to the theoretical values.

The machine we tested has two cores. We isolate one core by adding
\texttt{isolcpus=0} in bootloader configuration file. All the testing scripts
are run with \texttt{taskset -c 0\space./test} syntax. Therefore, we ensure minimum
number of system processes interfere with our testing code, thus minimizing
context switching overhead. Interrupts are also disabled during testing.
Finally, only one measurement is run at one time.

\subsection{Measurement Overhead}
The goal of this section is to report the overhead of reading system time and
the overhead of using loops to measure iterations of an operation.  These
measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
we used and modified the \texttt{cycle.h} package from FFTW\cite{FFTW}.  The
\texttt{cycle.h} is a superset of the rdtsc which optimizes according to
different machines.  To read the time stamp counter, use \texttt{getticks()}
function. To calculate the difference of two time stamp, use \texttt{elapsed}
function, which returns a double precision value.  To measure the overhead
time for reading time, we simply executed \texttt{getticks()} twice in a row
and took the difference. The idea behind this methodology was to start
tracking immediately before the the execution of the next command and stop
tracking as soon as the command is executed. The difference between the two
\texttt{getticks()} times will give us the number of ticks taken between the
two methods. Similarly, measuring \texttt{elapsed()} required that we add two
\texttt{getticks()}, one before and one after, the \texttt{elapsed()} method.
We then determined the difference between the two \texttt{getticks()} methods
that preceded and followed the \texttt{elapsed()} method. Taking that
difference, we subtracted the time taken for one \texttt{getticks()} method to
account for the time necessary to execute the succeeding \texttt{getticks()}
to arrive at our final result.

To measure the overhead time for using a single loop, we chose to measure
a \texttt{for} loop that will increment a variable from 0 to 9999, and
divided it by 10000 to obtain the overhead taken by a single iteration, which
includes incrementing and checking. In order to measure purely the looping
part of the \texttt{for} loop, we put the variable declaration before the
first \texttt{getticks()} method. This way, we measured the time taken by the
loop to check and increment the variable. We also took into account the time
taken by the \texttt{getticks()} method and subtracted it from our final time.
Our motive behind choosing to test the \texttt{for} loop comes from our own foresight
of using \texttt{for} loops plentifully in future tests.

Each test was ran 10000 iterations, the mean was taken.  Each experiment was
ran 100 times, and the average and standard deviation were taken.

In order to accurately translate "ticks" into human time, a preliminary
measurement of the correlation is made between ticks and number of cycles is
1 to 1.  Since each cycle takes 0.416 ns, given our CPU frequency, we can make
the correlation that 1 tick is equivalent to 0.416 ns.
\subsubsection{Prediction}
We originally predicted that, for \texttt{getticks()}, the operation should be
less than 20 instructions. Assume CPI is 1, it would take the hardware less
than 20 cycles to finish this instruction. And elapsed should take a little
more time to execute since it has subtraction, while \texttt{getticks()} is
simply reading data. Since the OS has to manage procedure call and returning
values, we think software overhead should be less than 5 cycles.

For \texttt{for()} loop, with one arithmetic operation, one comparison and one
jump, it should take about 3 cycles. The OS will have to initialize the loop
counter, which should be less than 2 cycles.

\subsubsection{Experiment Results}

The experimental results are presented in Table~\ref{table:rdtsc_overhead}.

\begin{table}
  \caption{Reading overhead performance (in cycles)}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation             & Hardware Est.         & Software Est.         & Prediction            & Mean          & Std. Deviation     \\ \hline
    \texttt{getticks}     & \textless20 (8.3 ns)  & \textless5 (2.1 ns)   & \textless25 (10.4 ns) & 45 (18.5 ns)  & $9*10^4 (3.6*10^4 ns)$    \\ \hline
    \texttt{elapsed}      & \textless25 (10.4 ns) & \textless5 (2.1 ns)   & \textless30 (12.5 ns) & 36 (14.4 ns)  & 0.5 (0.2 ns)    \\ \hline
    \texttt{for} loop     & 3 (1.2 ns)            & 2 (0.8 ns)            & 5 (2.0 ns)            & 7 (2.8 ns)    & 0.02 (0.008 ns)    \\ \hline
  \end{tabular}
\label{table:rdtsc_overhead}
\end{table}
\subsubsection{Result Discussion}

The predicted performance was less than the measured performance. Our
predicted performance was based on the naive assumption that one task, such as
a jump, would only take 1 cycle to perform. However, looking at the measured
operation times, our assumption was proven to be false. This means that the
operating system and hardware were taking more cycles to perform each task.
A possible reason is that the compiler translate one line of code into several
instructions. Another reason might be CPU scheduling and context switching
between processes.

To note, both \texttt{getticks()} and \texttt{elapsed()} functions are
modified to be of inline attribute. Therefore, the measurement would eliminate
the overhead of procedual calls. On the other hand, the measured operation
time was close to the predicted operation time for the \texttt{for()} loop.
The difference here resided in our overlooking one or two operations while
making our prediction.

Our methodology centered around measuring the simplest possible operation for
reading time and for looping. Because we measured time immediately before and
immediately after the operation of interest, we successfully captured the
entirety of the operation. With this information, we will be able to use it as
a base from which we can accurately measure other performances.

\subsection{Procedure Call Overhead}
The goal of this section is to report the overhead of making a procedure call
with arguments. The number of arguments the procedure takes in will also be
taken into account in a series of tests. Each test will take in different
amounts of arguments ranging from 0 arguments to 7 arguments. These
measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a procedure call, we measured the time to make
a call to a procedure that takes in the parameters given and does not
processing thereafter. The procedure itself does not return anything either.
In order to take into account the different time costs as a result of varying
amounts of parameters, we made procedure calls with 0 to 7 parameters. Each
procedure call with different amounts of parameters was tested 1000, and the
average and standard deviation were taken. Each procedure call was precluded
and succeeded by two different \texttt{getticks()}. Because of this, we also
took into account the amount of overhead \texttt{getticks()} populated and
subtracted that from the total in each procedure call.

Each of the procedure call did no processing of its arguments nor did it have
a return statement. The decision to make the procedure call in this way was
motivated by our goal of obtaining purely the overhead of making the call to
a procedure. It was decided that return statements and any processing of data
would not be part of the procedure call itself, and thus not included as part
of the testing processes.

\subsubsection{Prediction}
The number of inputs and size of inputs should both affect the time it
take for a processor call. Without inputs, a procedure should simply be two
jump instructions. Thus, we predict that the hardware cost of procedure call
with no inputs should be about 2. The system needs to remember where the
processor is called so that it can return, which should only take 1 cycle. As
input size grow, more operations will need to be done to copy the
input.

\subsubsection{Experiment Results}

\begin{figure}[!htb]
\centering
\includegraphics[scale=.6]{proc.eps}
\caption{Procedure call performance (reading overhead included)}
\label{fig:proc}
\end{figure}

\subsubsection{Result Discussion}
An interesting discovery is that, the relationship between number of
parameters and the time to issue a procedure call is not a strict linear
relation. This means that in one cycles, the system is able to execute more actions than one, which was what we had predicted.
It appears that, every 4 integer parameters added will add one extra cycle of
overhead of a procedure call. Thus, we can get to the conclusion that the os
is able to copy 128 bits of data at the same time.

Notice that we did not subtract the reading overhead in the graph, so the
actual overhead of procedure call should be 4~6 cycles. Thus, the overhead of
a procedure call is quite small. If the parameter size is large it will add to
the overhead, but still pretty fast.

\subsection{System Call Overhead}
The goal of this section is to report the overhead of making a system call and
compare it to the cost of making a procedure call. The system call with the
least overhead was chosen to accurately capture the time taken to make the the
switch from user mode to kernel mode. These measurements are important and
necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a system call, we attempted to choose a method
with as little overhead as possible. Initially, our goal was to used
\texttt{getpid()} as our system call of choice. However, due to the potential
problem of having the \texttt{getpid()} system call getting cached and thus
not having an accurate overhead time of trapping into the kernel space, we
decided to choose a similar method, \texttt{getppid()}. In order to measure
the time of \texttt{getppid()}, we went about by simply adding
\texttt{getticks()} before and after the system call and calculated the
average difference and its standard deviation over 10000 runs. This method was
similar to the methodology described earlier in measuring loop overhead. We
then compared the results that we received to the results we measured earlier
for procedure calls.

\subsubsection{Prediction}

\subsubsection{Experiment Results}
The experimental results are presented in Table~\ref{table:systemcall_overhead}.
\begin{table}
  \begin{center}
    \caption{System Call Overhead (in cycles)}
      \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        Operation   & Prediction            & Mean                       & Std. Deviation     \\ \hline
        getppid()   & 15 (7 ns)             & 242.35 (96.94 ns)          & 9.06 (3.624 ns)    \\ \hline
      \end{tabular}
  \end{center}
\label{table:systemcall_overhead}
\end{table}

\subsubsection{Result Discussion}
In the process of running the experiments initially with getpid(), we noticed the effects of caching in skewing the measured results. This was something we hoped to avoid because the caching process would signify that trapping into the kernel will happen only during the first time the method is called. We went forth with getppid() because it proved to have much more consistent results in the initial and succeeding trials. The inaccuracy found in our predictions can largely be related to our lack of knowledge regarding the entire set of steps taken when trapping into the kernel. There can also be some extra processing needed in retrieving the parent id, such as verification, that we naively did not acknowledge. In comparison to procedure calls, system calls take almost 4 times the amount of time. This is due largely to the switching into kernel space and all processing that happens thereafter, such as validation, before the operation can go on.

\subsection{Task Creation Time}
The goal of this section is to report the time taken to create and run both
a process and a kernel thread, and to compare the two time costs.

\subsubsection{Experiment Methodology}
In order to measure time taken to create and run both a process and a kernel
thread, we used the basic principle of precluding and succeeding the target
section of code as we have prior. In order to create a kernel thread, we used
the method \texttt{pthread\_create()} to create and run a thread.

\texttt{pthread} allows us to avoid having to write kernel code and this
flexibility was one of the reasons why we decided to use it. The other reason
is that the thread is created executing any argument given as part of its
parameter. This means that we will not need to explicitly call any end
operation on the thread.

The arguments given to the \texttt{pthread\_create()} method were defaulted as
to avoid overhead that is separate from the task creation and minimal amount
of running. This test was repeated 10000 times, and the average and standard
deviation were taken while taking into consideration the overhead of reading
time. We used the same procedures for processes. We created processes using
\texttt{fork()} and terminated each process using \texttt{\_exit()}
immediately after creation and running.  The times were taken immediately before creation and another time was taken immediately after the process returns from \texttt{\_exit()}.Each experiment has 10000 iterations,
we run 100 experiments for process creation, and 1000 for thread
creation(since process creation is too slow) and the average and standard
deviation were taken while taking into consideration the overhead of reading
time.

\subsubsection{Prediction}
The time needed to create a task is really difficult to predict, since it
really dependents on how OS define a task, which means details such as process
data structure, thread data structure, implementation algorithms, will all
affect the time. Since we have little knowledge of linux kernel, we don't
think we can make a reasonable guess. One thing we would expect is that, the
time to create a process should be longer than creating a thread, since
processes are more heavy weighted than thread.

\subsubsection{Experiment Results}
\begin{table}[h]
  \begin{center}
    \caption{Thread creation performance (reading time included)}
    \begin{tabular}{|l|l|l|}
      \hline
      operation            & Mean of experiments       & Standard deviation       \\ \hline
      create process       & 209194.13(87.02 microsec) & 28245.38(11.75 microsec) \\ \hline
      create kernel thread & 3030.83(1.25 microsec)    & 68.57(28.53 ns)          \\ \hline
    \end{tabular}
    \label{table:thread_creating_results}
  \end{center}
\end{table}


\subsubsection{Result Discussion}
From the result, we could observe a significant difference between the time it
took to create a process and a kernel thread. We believe it is because
processes have much more complicated structure than threads. For example,
threads share memory while processes have separate address spaces. Such
a difference is reasonable in the sense that, threads are the minimum unit of
scheduling, and processes take advantage of threads to get better performance.
One more thing to mention, we actually measured the time it take for a kernel
module to create a kthread, and it seems to be even more time consuming than
process creation. We are wondering why would that happen, since a kernel
module is already in kernel level, it won't need to cross the boundary of user
level and kernel level again, why would it take so much time? Not to mention
it is a thread not a process.

\subsection{Context Switch Time}
The goal of this section is to report the time taken to context switch from
one process to another in comparison to the time taken to context switch from
one kernel thread to another.  A context switch in essence is the process of
storing current process state and restoring another. More specifically,
storing a state means changing the process's state to ready or blocked;
restoring a state brings the process from ready to running.

The overhead time spent in a context switch consists of several parts.  First
and foremost, by definition time needs to be spent saving and restoring
a process's states.  Secondly, process caching may also influence the overhead
time. Thirdly, virtual memory mapping, synchronization of memory caches,
paging, these elements also may not be ignored when switching between
processes. Finally, interrupts may occur in the middle of measurement.

In this paper, we mainly compare a process-level context switch and a kernel
thread-level context switch.

\subsubsection{Experiment Methodology}
In order to measure the time taken to make a context switch between two
processes, we utilized blocking pipes to ensure synchronization. The way we
designed the test to measure just the time for context switching required the
use of the pipe's ability to pass data.

At first, we tried to use a simple \texttt{fork} followed by and \texttt{exit}
in the child process, and use \texttt{waitpid} in the parent process in order
to achieve synchronization. The implementation of this method is trivial, but
it turned out to generate irregular results this methodology was abandoned. Forking child processes within
a loop easily drains memory, while using \texttt{waitpid} adds a unstable
overhead to the measurement.

For user-level process context switching, we eventually decided to use a pipe
mechanism to ensure synchronization. We first spawned a child process
, wherein a time was taken before the fork() call, and a \texttt{write()} procedure was done in conjunction with a pipe with which the time into the pipe as part of its arguments. The child process
immediately calls exit after and a context switch is made back to the parent
process.  Within the parent process, we immediately call a \texttt{read()}
procedure and store the first time taken in the child process.  A second time
is taken right after and the difference is taken. In the parent process, we
used \texttt{waitpid()} to ensure the following order of execution with the correct parent and chil: parent, child,
parent. Because pipes forces context switch into the kernel, the significant
overhead of the pipe's \texttt{write()} and \texttt{read()} operations were also
measured and taken into account in the final calculations since they are essentially not part of what we are attempting to benchmark. This overhead was benchmarked by performing multiple \texttt{write()} and \texttt{read()} operations with pipes, and an average was taken over the number of times this was performed. Each context switch
was performed 10000 times and an average was taken. This measurement tool was
ran 10000, and an average and standard deviation was taken for the final
results.

For kernel-level thread context switching, we also used pipe to force two
threads to synchronize.  First, we created two pipes p1 and p2. In both of
pipes, define the first integer as reading, and the second integer is writing.
Then, we use \texttt{pthread\_create()} and \texttt{pthread\_join()}to create
two threads: Ta and Tb. Set the pthread scope
\texttt{PTHREAD\_SCOPE\_SYSTEM}to ensure that this thread is a kernel thread.
After this, we timestamp the initial time t0 in the start of thread Ta.  Let
both threads communicate to each other though the two given pipes. Ta writes
data to p1 and listens to p2; Tb writes data to p2 and listens to p1.  The
communication is repeated 1000 times in both processes.  In the end of Ta, get
time stamp t1. The time duration $dt = t1-t0$ includes 2000 context switches,
so we can get the average context switch overhead by $dt/2000$. This precess
is repeated
100 times.

\subsubsection{Prediction}

The prediction for context switching is not easy. But we can be sure that
a kernel-level thread context switching would spend way less time than
a process one.

For process context switching, a process must call \texttt{wait}, jump into
kernel mode, then the kernel picks the right process to wake it up, pass
parameters. If the process is switched out to memory, hardware may also add
extra overhead.
For kernel threads, since threads share the same file descriptors, signals,
etc.\ , we estimate a smaller hardware overhead.


\subsubsection{Experiment Results}

The results can be found in table~\ref{table:contextSwitch_overhead}.

\begin{table}
	\begin{center}
  \caption{Context switching overhead performance (in cycles)}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation          & Hardware Est.   & Software Est.         & Prediction            & Mean          & Std. Deviation     \\ \hline
    Process            & \textless50000  & \textless20000   & \textless60000 & 71582.43 (0.0286 ms)  & 507.32 (0.001 ms)    \\ \hline
    Kernel Thread      & \textless10000  & \textless10000 & \textless 20000 & 23425.98 (9760.8 ns) & 211.34 (88.058 ns)  \\ \hline
  \end{tabular}
\label{table:contextSwitch_overhead}
\end{center}
\end{table}

The results are pretty much similar to the predictions.

\subsubsection{Result Discussion}

Forcing a context switch is not simple. When measuring process-level context
swiching, at first we only used forking to test the overhead, but the results
were very irradical.

During implementation of thread-level context switching, we first tried using
a mutex and wait-and-signal mechanism to preserve synchronization. However,
This method leads to complex structures and to the necessity to test overheads
of using semaphores.  Using the pipe mechanism forces the context switch
orders without much complexity, and it's more trackable to debug.


%-----------------------------------------------------------------------------
% Milestone 2: Memory
%-----------------------------------------------------------------------------

\section{Memory}
\subsection{RAM access time}
\subsubsection{Experiment Methodology}
Our methodology was planned based on the experiments that were done in the lmbench paper. In order to measure the time required to access the two caches and main memory with the least amount of overhead, we chose to do memory accesses using integer arrays. We created arrays of different sizes ranging from KB to 1GB with different strides ranging from 512B to 32KB. With each combination of array size and stride length, we performed 1000000 integer accesses just as the lmbench paper did. While they performed p = *p; in their paper, we did the C++ respective version. Some of the problems that we encountered in our creation of the methodology included pre-fetching. In order to bypass this behavior, we followed the lmbench paper and performed random accesses to the array. In order to bypass the possibility of our random number generator generating the same number, we preloaded integers within the array itself that acts as the location of the next integer access in the array that also takes into account the stride. When running the code, the initial location is randomly accessed. The time of these 1,000,000 accesses were then averaged and given as the time of access for that particular combination of array size and stride length. All time calculations took into account the overhead times involved. The entire experiment was done 10 times, and the average is show in the results below.  

\subsubsection{Prediction}
From previous experiments, we noticed how difficult it was to estimate the exact numbers for time of access. Therefore, our prediction for this part will be done on what we believe the graph will look like. Based on the fact that accessing the L1 cache is several times faster than accessing the L2 cache, we believe that there will be at least two plateaus and two sharp inclines on the graph. The inclines represent the increase in time necessary to access the L2 cache and the main memory. Based on the hardware specifications, these two inclines should be at the 32KB and 3072KB mark. The plateaus will be a result of the time to access memory from within the cache. Because we are dealing with larger amounts of memory in larger quantities, the numbers were also predicted to be much larger than what we have seen in previous experiments. We also expect a small amount of latency required to access elements in arrays of sizes smaller than the L1 cache. We expect to be about 4 cycle's time or about 1.6 ns. 

\subsubsection{Experiment Results}
\begin{figure}[!htb]
\centering
\includegraphics[scale=.5]{cahce_access.png}
\caption{Latency for individual integer accesses to main memory and the L1 and L2 caches}
\label{fig:cache_access_time}
\end{figure}

\subsubsection{Result Discussion}
In the results graph, we can see a clear staircase shape similar to what we have expected. We see that accessing memory less than the size of the L1 cache (from 10 to 15 on the graph's x-axis) is constant. The increase in time at 32KB shows that we are now accessing memory outside of what the L1 cache can hold. The relative plateau formation from 32KB to 3072KB (from 15 to 21 on the graph's x-axis) shows the latency of accessing L2 cache. Then there is an extreme incline in latency thereafter which represents the difference in time needed to access the main memory compared to the L2 cache. 

An unexpected result was the small dip found in the 128KB region. This may be a result of the possible situation where the stride is not large enough in comparison to the array size. This would mean that the next integer to access would be within some bytes range that was already cached. We can see these smaller dips prevalent throughout the entire graph in different place and in varying degrees. Therefore, for the analysis of our methodology, our method was not completely perfect and there were situation as such that we did not consider. However, for the most part, the methodology is correct. The result shows the trend that we have expected and also matches with what McVoy et. al. reported in the lmbench paper.


\subsection{RAM bandwidth}
\subsubsection{Experiment Methodology}
In order to measure the bandwidth of memory, we allocate different size of array in memory, then read the whole array sequentially and measure how fast we can read. \\
On this particular measurement, we implemented our experiment both in c and assembly. We used nasm as the assembler, with the following compiling command:\\

nasm -f elf <source.asm> -o <target.o> \\
gcc -funroll-loops -m32 -mtune=pentium <target.o> <source.c> -g -lm -o <target>\\

We have the assembly function read the array 100 times consecutively, then free the array and recall the assembly function. We repeatedly call the assembly function 100 times. Thus, we read the array for 10000 times in one experiment. We output the mean time it takes to read the array. Then we execute the experiment 100 times, get the mean value and standard deviation of these experiments. In this process, we've already subtract the getticks overhead from our experiment result\\
In order to avoid our read being cached, we used the Intel SSE4 streaming load assembly operation ``movntdqa'', which read 16 bytes of data in one operation.\\

\subsubsection{Prediction}
According to our hardware specification, our maximum memory bandwidth should be:\\
$$ 667000000 * 2 * 64 / 8 = 10.672GB/s $$
Thus, we predict that our memory read bandwidth will be less than this value.\\

\subsubsection{Experiment Results}
Our results are shown in Figure \ref{fig:ram_bw}. and Table \ref{table:mem_bw} \\

\begin{table}
	\begin{center}
  \caption{Memory read bandwidth(100 experiments)}
  \begin{tabular}{|l|l|l|}
  \hline
Read size& Bandwidth(MB)& standard deviation \\  \hline
128B  & 30246.5123374 & 2463.99759062 \\  \hline
256B  & 32385.0898914 & 985.005918514 \\  \hline
512B  & 32962.138536  & 334.818111688 \\  \hline
1KB   & 34406.4881076 & 1022.40808475 \\  \hline
2KB   & 35220.0924862 & 851.44763812 \\  \hline
4KB   & 35485.9973028 & 1013.02041157 \\  \hline
8KB   & 35939.0685171 & 907.868248426 \\  \hline
16KB  & 35574.1324265 & 1718.20388877 \\  \hline
32KB  & 31544.0219404 & 1407.52979557 \\  \hline
64KB  & 16172.9533499 & 744.0446469 \\  \hline
128KB & 16450.7229957 & 270.207782424 \\  \hline
256KB & 16519.9494    & 55.4321995455 \\  \hline
512KB & 16525.8607228 & 60.234820764 \\  \hline
1MB   & 16494.4736645 & 124.302969813 \\  \hline
2MB   & 13618.2029798 & 1008.70403144 \\  \hline
4MB   & 6114.52143216 & 126.829869046 \\  \hline
8MB   & 5322.41277172 & 29.9100826471 \\  \hline
16MB  & 5316.23314084 & 27.867507391 \\  \hline
32MB  & 5314.78016467 & 37.4761378191 \\  \hline
64MB  & 5311.64655261 & 55.3834994529 \\  \hline
  \end{tabular}
\label{table:mem_bw}
\end{center}
\end{table}


\begin{figure}[!htb]
\centering
\includegraphics[scale=.5]{RAM_bw.eps}
\caption{Left y-axis is bandwidth, right y-axis is std/mean, x-axis is the size of memory read}
\label{fig:ram_bw}
\end{figure}

\subsubsection{Result Discussion}
First of all, from the graph we can clearly see that it is obvious ``movntdqa'' operation did not bypass cache at all. The drop in 32KB size and 3MB size are due to the L1 cache and L2 cache which has exactly the corresponding size. Despite the fact we did not by pass cache, we still believe the memory bandwidth we measured when reading data that has a size over 4MB is a reasonably good approximate of the actual memory bandwidth, which is about 5.3GB/s.\\
There are several reason that this value is a reasonable approximate: First, since are array size is large enough, every time we repeat the reading the array, the array data will not be in cache any more, cpu will need to get them from memory anyway. Second, although we are aware that cpu will read one whole cache line(64bytes) at a time, we think that, the time of 1 read 64 bytes(which is essentially one ``movntdqa'') from memory followed by 3 ``movntdqa'' 16 bytes read from cache, will not be significantly longer than 1 read of 64 bytes from memory, since cache access speed is significantly faster than memory access.\\
One other overhead we did not take into consideration is virtual address translation time. We did not figure a good we to measure and subtract that from our result. We believe that would be the main cause of error in our result.\\
It is reasonable to see how standard deviation bump up when cache boundary is reached. Also, the bump of standard deviation when array size is small indicate that, when reading very small data, the function is more vulnerable to disturbance of other processes and cpu scheduling in the system.\\

\subsection{Page fault service time}
A page fault occurs when the OS tries to fetch a page that was swapped out from the physical memory, and thereby it needs a further fetch from the disk. Measuring the page fault service time means measuring the time interval between a page fault occurance and loading the page from the disk.
\subsubsection{Experiment Methodology}
One neat way to conduct the measurement is to use mmap (memory mapped files). The mmap function is a system call that directly maps a file to the virtual memory space of the calling process.Upon mapping, the actual data won\'t be copied into main memory until accessed by the process. This is because the mmap approach is using lazy loading, which makes it perfect for our measurement of a page fault service time. 

The experiment is devided into two steps; first part is to prove that a page fault occurces when sequentially accessing the file; second part is to calculate the average service time during a page fault.
Both of the experiments are run against a dummy file we created with the same repeated letters, size 400KB. The page size of the system is 4KB, which is got from a sysconf system call.

In the first experiment, since the mmap returns a pointer to the starting address where the file is mapped in the virtual memory, we iterate the pointer through all letters in the file, that is we are accessing one byte at a time. Since the file is loaded into the physical memory page by page, this would force a page fault accessing the file every 4KB (size of one page in the system). We decide to do a backward scan on moving the pointer in case of a potential read-ahead.

In the second experiment, we set the scan stride to be 4KB. This forces a page fault every byte of access in that mmap loads the file from disk to physical memory page by page, thus gives us a measurable results to calculate the average. 
We iterate 10,000 times mmap-ing and munmap-ing the file, and access a byte every 4KB to the file. Since the size of the file is 400KB, this gives us 1,000,000 samples. 
One thing to notice is that to repeat the measurement,  the file may be kept in the file system buffer cache. Therefore, the buffer cache should be flushed out during each iteration to get an accurate result.

\subsubsection{Prediction}
Each page fault implies loading 4KB from the disk, which should spend much longer than directlly accessing physical memory. We estimate this process to around This should take from 8 to 10 ms on the bare hardware. The software overhead may add 1ms. 
See table~\ref{table:pagefault} for data.

\subsubsection{Experiment Results}


\begin{figure}[!htb]
\centering
\includegraphics[scale=.45]{pagefault_sequential.png}
\caption{Sequential Access mmap file}
\label{fig:pagefault_sequential}
\end{figure}

\begin{table}
	\begin{center}
  \caption{Context switching overhead performance (in cycles)}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation             & Hardware Est.         & Software Est.         & Prediction            & Mean        & Std. Deviation       \\ \hline
    mmap file accessing   & 10ms                  & 1ms                   & 11ms                  & 7.4ms       & 1.1 ms               \\ \hline
  \end{tabular}
\label{table:pagefault}
\end{center}
\end{table}

See table~\ref{fig:pagefault_sequential} for first part of the experiment.\\
See table~\ref{table:pagefault} for second part of the experiment.


\subsubsection{Result Discussion}

Dividing the result by the size of a page, which is 4KB, we get an average of 1.9ms accessing a single byte by a page fault. This is 40,000 times as big as the The latency of accessing a byte from main memory.

%============================= section ==========================%
\section{Network}
%----------------------------- subsection --------------------------%
\subsection{Round Trip Time}
\subsubsection{Experiment Methodology}
Round trip time measures the time spend for a established tcp connection to send one packet and receive an acknowledgement of that packet. AKA, the time for the packet to do a "round trip".\\
In order to measure it, we used c language server-client flavored socket program to establish connection, transmit and measure time. We measure the time it took on a established connection for one sigle write of one byte of data, and one single read of one byte of data implemented using the write() and read() function on an established socket connection. We measured one packet roundtrip time on a established link for 100/1000/10000/100000 times to observer different roundtrip time. Then we break that connection, repeat the experiment for 100/10 times, to get a mean and standard deviation. The number of experiments we did is based on the stability of the measured data. When standard deviation is small enough, we consider the data is stable enough.\\
Since round trip time depends heavily on the internet connection environment between the server and client, we will describe our network environment in the next part. \\
In terms of ICMP, we also used c socket to perform ping operation. We meaure the time between sending and receiving the ICMP packets, repeat this for 1000 times for each experiment, and repeat the experiment for 100 times just like the roundtrip experiment. \\
\subsubsection{Experiment Setup and Prediction}
Since complex network routing will significantly affect the stability of experiment, we used direct cable connection for our experiment. \\
The specification of the other machine used here is give in form \\ %TODO: add the spec form here


\subsubsection{Experiment Results}
\subsubsection{Result Discussion}

%----------------------------- subsection --------------------------%
\subsection{Peak Bandwidth}
\subsubsection{Experiment Methodology}
The prerequisite to measure the peak bandwidth of one machine is that, the other machine which recevives the data, and the network between them, has to be capable of transfering data faster than the peack bandwidth of the NIC of that particular machine. In order to satifiy this precondition, we first chose another machine which is labled to have a better hardware performance, then measure the bandiwdth between them, and the bandwidth between the better machine with an even better machine, to make sure that it is not the bottle-neck of the peak bandwidth of the target machine. In order to rule out the bottle-neck of router, we used direct cable connection.\\
In terms of implementation, we test the bandwidth by sending data over TCP. For each test, we send 10MB of data and measure the time used to send it, then calculate the bandwidth. Each test is repeated 100 times, each experiment is repeated 100 times.\\
\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}

%----------------------------- subsection --------------------------%
\subsection{Connection Overhead}
\subsubsection{Experiment Methodology}
To measure connection overhead of TCP protocol, we did not count the time to obtain a local socket as the overhead, we only measure the time used to establish the connection.
\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}


%============================= section ==========================%
\section{File System}

%----------------------------- subsection --------------------------%
\subsection{Size of cache}
\subsubsection{Experiment Methodology}
In order to estimate the size of the file cache, we created a variety of files with sizes ranging from 50MB to 2GB. With each different file, we measured the time it took to read the file over an average of 10 times per file. This idea behind the methodology comes from the idea that attempting to read within a file that fits within a cache will have relatively the same read time. However, as soon as we have a file whose size is larger than the cache size and we try to access a part of the file outside of the cache, the read time will be much larger. Therefore, we attempted to read from a variety of file sizes in order to capture the size at which the large read time occurs. The large jump in read time will indicate to us the size of the file cache of the system. 

\subsubsection{Prediction}
We can safely say that the size of the cache will be smaller than main memory, which is 2GB. Since the OS will take a large portion of the main memory for file system cache, we predict that the file cache size will be less than half of the size of main memory, <1GB. 

\subsubsection{Experiment Results}

\begin{figure}[!htb]
\centering
\includegraphics[scale=.6]{file_cache_size.png}
\caption{Left x-axis is the size of file and y-axis is average read time. Both are on log scale.}
\label{fig:file_cache_size}
\end{figure}

The results can be found in figure~\ref{fig:file_cache_size}.

\subsubsection{Result Discussion}
The results show that there is a small increase in read time at 200KB. From there, we have a very small increase in read time until 776MB (2\textsuperscript{19.6} KB). The read time then begins to level out again towards the end after 1.5GB (2\textsuperscript{20.6} KB). This indicates to us that the file cache size is a little larger than 776MB. We are able to deduce this from the relationship between file cache hits and the size of the file. If the file is able to fit within the cache, then we will be reading from caches and thus have a relatively stable reading time for all files of sizes that fit within the cache. Those files that are too large to fit in the cache, however, will cause cache misses and thus longer time to fetch from outside of the cache. This increase in time can be seen at around 776MB, which indicates to us that this is the relative size of the file cache. 

Overall, our methodology was successful in helping us attain the information we required. However, to pinpoint the size of the cache more clearly, future experiments will benefit form decreasing the rate in which we are increasing the file sizes. As vague as our estimates may have been, our estimates were accurate relative to the information attained from the experiment. The experiment does, however, give us a fairly loose range from which we can expect our cache size to be.

\subsection{File read time}

\subsubsection{Experiment Methodology}
The basis of our experiment is to perform both sequential and random reads from files of different sizes, which we made to range from 8KB to 200MG. The purpose of this experiment was to compare and contrast the read time for sequential and random reads. In order to do so, we needed to use a raw device to ensure that we were not measuring cached data. As a point of clarification, a raw device is a special block device file that bypasses the OS's caches and buffers. However, raw devices were deprecated on Linux systems, therefore we alternatively used the \texttt{O\_DIRECT} and \texttt{O\_SYNC} flags associated with the \texttt{open()} system call. The \texttt{O\_DIRECT} flag ensures that the file is not brought into the file buffer cache, and every read goes to the disk. The \texttt{O\_SYNC} flag ensures that the reads were performed synchronously. 
We also verified that the file is not brought into the cache by observing the output of the \texttt{vmstat} command in linux. For reading sequential blocks, we simply used read() and put time markers for benchmarking around the read commands for reading an entire file. The random reads were harder to perform because we had to indicate where to fetch the next block. In order to do so, we used lseek() along with read(). The overall methodology for benchmarking random file accesses was the same as sequential accesses except for the need to use lseek() to indicate the amount to bytes to offset before starting the read. All reads for different file sizes were done 100 times and the average was taken per block.

\subsubsection{Prediction}
We expected the time for random read to be much larger than the time for sequential read. Sequential reads can benefit from locality and pre-fetching to a greater extent than random reads will be able to. Therefore unless the order in which random reads were made was almost the same as sequential reads, then random reads should be significantly larger than sequential reads. We also suspect that the read time for sequential reads to have a downward trend at some point as a result of benefiting from reading blocks that are sequential. We predicted that the hardware would take about 6600 ns, which is the hardware's average latency and track to track seek time. For random access time, we added the average seek read time to this for a total of 20600 ns of estimated hardware time.The software for both was simply an estimated time that it would take for the command to propagate to the hardware.

\subsubsection{Experiment Results}

The results can be found in figure~\ref{fig:file_read_times}.

\begin{figure}[!htb]
\centering
\includegraphics[scale=.6]{file_read_times.png}
\caption{Left x-axis is the size of file and y-axis is average read time per block. Both are on log scale.}
\label{fig:file_read_times}
\end{figure}


\subsubsection{Result Discussion}
We will now consider the results of the experiment as it currently stands. For the sequential read, we see that there is a slight decreasing trend initially. This was what we had anticipated in our estimations where we see that the sequential reads are taking advantage locality and pre-fetching. However, we don't see the same pattern in random reads because of the randomness with which we area accessing the files' data. We see a slight increase in both sequential and random file reading at around 2\textsuperscript{9.6}KB and an even more drastic increase in time at 2\textsuperscript{19.6}KB. The larger increase is due to files exceeding the size of the file cache, which we did not take into account in our estimation. This increase is more drastic for random reads because we are essentially making random reads from disk. This means that there will be extra time needed to find the different blocks on disk. Compared to sequential read which doesn't require jumping around to seek on the disk, there is a larger increase in read time for random reads once the file exceeds cache size.

Our methodology was successful overall as we can see the difference in time needed to make sequential and random read from different sized files. However, there is room for improvement. Had we ran the benchmarking for read times with a larger upper bound, we can expect to see when the read time for random reads will begin to plateau. This is the point in which the read time for a block will be dominated mostly by random read time from disk. Because we are measuring the average read over 100 iterations, we decided to end the testing prematurely due to time constraints. 

Sequential access may not be sequential if given enough time or if trying to access a large file. Even though the system attempts to place related blocks in approximately the same area or sequentially, this effort may prove to be more difficult when there is not enough room to place blocks sequentially. In such a case, the blocks will be placed within some distance from its previous block. If the file is large enough, this non-sequential placement of blocks could result in high levels of fragmentation and be less sequential.

\subsection{Remote file read time}

\subsubsection{Experiment Methodology}
\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}

\subsection{Contention}

\subsubsection{Experiment Methodology}
\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}


\section{Time Consumption}
\subsubsection{CPU, Scheduling, and OS Services}
Boyuan Qin: 16 + hours\\
Dexin Qi: 16+ hours\\
Qiheng Wang:16 + hours
\subsubsection{Memory}
Boyuan Qin: 16 + hours\\
Dexin Qi: 16 + hours\\
Qiheng Wang:16 + hours
\subsubsection{Network}
Dexin Qi: 20 + hours 
\subsubsection{File System}
Boyuan Qin: 20 + hours\\
Qiheng Wang: 


\section{Complete Overview}

\begin{table}[!htbp]
  \caption{Overhead Operation Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation              & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    \texttt{getticks}      & 8.3 ns                & 2.1 ns                & 10.4 ns               & 18.5 ns\\ \hline
    \texttt{elapsed}       & 10.4 ns               & 2.1 ns                & 12.5 ns               & 14.4 ns\\ \hline
    \texttt{for} loop      & 1.2 ns                & 0.8 ns                & 2.0 ns                & 2.8 ns\\ \hline
    System Call            & 4 ns                  & 3 ns                  & 7 ns                  & 96.94 ns\\ \hline
    Create Process         & 100 ns                & 100 ns                & 200 ns                & 11.75 ms\\ \hline
    Create Kernel Process  & 90 ns                 & 90 ns                 & 180 ns                & 28.53 ns\\ \hline
    Process Switch         & \textless50000 ns     & \textless20000 ns     & \textless70000 ns     & 0.1 mns\\ \hline
    Kernel Thread Switch   & \textless10000 ns     & \textless10000 ns     & \textless20000 ns     & 88.06 ns\\ \hline
  \end{tabular}
\label{table:overview_overhead}
\end{table}

\begin{table}[!htbp]
  \caption{Memory Operation Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation               & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    L1 RAM Access           & 1.6 ns                & 0 ns                  & 1.6 ns                & 3 ns\\ \hline
    L2 RAM Access           & 3.2 ns                & 0 ns                  & 3.2 ns                & 25 ns\\ \hline
    Main Memory RAM Access  & 32 ns                 & 0 ns                  & 32 ns                 & 275 ns\\ \hline
    Read RAM Bandwidth      & ns                    & ns                    & ns                    & ns\\ \hline
    Write RAM Bandwidth     & ns                    & ns                    & ns                    & ns\\ \hline
    Page Fault Service Time & 10 ms                 & 1 ms                  & 11 ms                 & 7.4 ms\\ \hline
  \end{tabular}
\label{table:overview_memory}
\end{table}

\begin{table}[!htbp]
  \caption{Network Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation               & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    ?????????????           & ns                    & ns                    & ns                    & ns\\ \hline
  \end{tabular}
\label{table:overview_network}
\end{table}

\begin{table}[!htbp]
  \caption{File System Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation               & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    File Cache Size         & \textless2GB          & n/a                   & \textless2GB          & 2\textsuperscript{19.6} to 2\textsuperscript{20.6}MB (about 1.13GB)\\ \hline
    Sequential Read Time    & 6600 ns               & 5 ns                  & 6605 ns               & 175125 ns\\ \hline
    Random Read Time        & 20600 ns              & 5 ns                  & 20605 ns              & 562180968 ns\\ \hline
    Remote Read Time        & ns                    & ns                    & ns                    & ns\\ \hline
    Remote Write Time       & ns                    & ns                    & ns                    & ns\\ \hline
    Contention              & ns                    & ns                    & ns                    & ns\\ \hline
  \end{tabular}
\label{table:overview_file}
\end{table}

%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\begin{thebibliography}{1}

\bibitem{FFTW} Cycle Counters. Available at {\tt http://www.fftw.org/download.html}.
\bibitem{}http://en.wikipedia.org/wiki/Penryn\_(microprocessor)
\bibitem{}http://tuxthink.blogspot.com/2011/02/kernel-thread-creation-1.html
\bibitem{}http://jahanzebnotes.blogspot.com/2013/02/turn-off-cpu-throttling-ubuntu.html
\bibitem{}http://www.seagate.com/staticfiles/support/disc/manuals/notebook/momentus/5400.6\%20(Wyatt)/100528359e.pdf
\bibitem{}http://www.cpuid.com/softwares/cpu-z.html
\bibitem{}Larry McVoy and Carl Staelin. 1996. lmbench: portable tools for performance analysis. In Proceedings of the 1996 annual conference on USENIX Annual Technical Conference (ATEC '96). USENIX Association, Berkeley, CA, USA, 23-23.
\bibitem{}http://codearcana.com/posts/2013/05/18/achieving-maximum-memory-bandwidth.html
\bibitem{}http://www.nasm.us/doc/nasmdoc6.html
\bibitem{}http://zsmith.co/bandwidth.html
\bibitem{}http://www.sisoftware.net/?d=qa\&f=ben\_mem\_latency
\bibitem{}http://www.bitmover.com/lmbench/

\end{thebibliography}

\end{document}
