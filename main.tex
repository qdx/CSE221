\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_09,times}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[T1]{fontenc}
%\usepackage{adjustbox}


\title{CSE221 Operating System Measurement}

\author{
  Boyuan Qin\\
  \texttt{bqin@eng.ucsd.edu}\\
  \And
  Dexin Qi\\
  \texttt{deqi@eng.ucsd.edu}\\
  \And
  Qiheng Wang\\
  \texttt{qiw018@eng.ucsd.edu}\\
}
\date{\today}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

%============================= section ==========================%
\section{Introduction}
The goal of this project is to measure the performance of the operating
system, Ubuntu 12.04.4 LTS\@.  The reason behind this goal is to gain an
understanding of the relationship between the underlying hardware and the
operating system, and their effects on how much time integral operations will
require.  With this goal, we will be running a series of justified experiments
that will allow us to characterize the speed of each operation and compare
them to each other.  We will use the course of these experiments as a way of
gauging our intuition about the performance of the operating system we are
testing.  The information we will obtain from these experiments will also be
used in future endeavors as a set of performance results to compare against.

The language of choice in these tests will be C/C++/Assembly.  The reason for choosing
C/C++ comes from the flexibility that C/C++ provides. It is a high-level language that allows us to, at the same time, work without much overhead as, for example, Java when measuring time. While assembly could provide more fine tuned experiments which can avoid being twisted by compiler. The compiler we are using is GCC 4.6.3-1ubuntu5.

%============================= section ==========================%
\section{Machine Description}

The machine we measured is the Samsung R710 Laptop bought in October 2008. Its specifications are described in Table 1.\\

\begin{table}[h]
  \caption{Machine Specifications}
  \begin{center}
    \resizebox{\columnwidth}{!}{
      \begin{tabular}{|>{\centering\arraybackslash\bfseries}m{1in}|l|l|}
        \hline
        \multirow{8}{*}{Processor}       & Model                                 & Intel\textregistered Core\texttrademark 2 Duo Processor P8600  \\ \cline{2-3}
                                         & Instruction Set                       & 64 bit                                                         \\ \cline{2-3}
                                         & Cycle Time                            & 0.417ns(2.4GHz frequency)                                      \\ \cline{2-3}
                                         & L1 data cache                         & 32KB per core, 8-way set associative, 64-byte line size        \\ \cline{2-3}
                                         & L1 instruction cache                  & 32KB per, core 8-way set associative, 64-byte line size        \\ \cline{2-3}
                                         & L2 data cache                         & 3072KB, 12-way set associative, 64-byte line size              \\ \cline{2-3}
                                         & FSB                                   & 1066 MHz                                                       \\
        \hline
        \multirow{11}{*}{Hard Drive}     & Model                                 & Seagate Momentus\textregistered  5400.6 SATA model ST9500325AS \\ \cline{2-3}
                                         & Capacity                              & 500GB                                                          \\ \cline{2-3}
                                         & Cache                                 & 8 Mbytes                                                       \\ \cline{2-3}
                                         & RPM                                   & 5400                                                           \\ \cline{2-3}
                                         & Physical heads                        & 4                                                              \\ \cline{2-3}
                                         & Discs                                 & 2                                                              \\ \cline{2-3}
                                         & Average Seek Read                     & 14ms typical                                                   \\ \cline{2-3}
                                         & Full Stroke Seek                      & 30ms                                                           \\ \cline{2-3}
                                         & Average Latency                       & 5.6ms                                                          \\ \cline{2-3}
                                         & Track to track seek time              & 1ms typical                                                    \\ \cline{2-3}
                                         & I/O data transfer rate                & 300 Mbytes/s max                                               \\
        \hline
        \multirow{3}{*}{Memory}          & Capacity                              & Dual channel(symetric), each 1024 MBytes                       \\ \cline{2-3}
                                         & Frequency                             & DDR3 PC3-10700 667 MHz                                         \\ \cline{2-3}
                                         & Width                                 & 64 bit per channel                                             \\
        \hline
        \multirow{2}{*}{Network Card}    & Model                                 & marvell yukon 88e8055                                          \\ \cline{2-3}
                                         & Data Transfer Rate                    & 10/100/1000Mbps                                                \\
        \hline
        \multicolumn{1}{|>{\bfseries}c|}{\multirow{2}{*}{OS}} & \multirow{2}{*}{ Linux Distribution } & Ubuntu 12.04.4 LTS                        \\
                                                              & \multicolumn{1}{c|}{}                 & GNU/Linux 3.8.0-35-generic i686           \\
  % here if use |c| instead of c|, will generate an extra vertical line
        \hline
      \end{tabular}
    } % end of resizebox
  \end{center}
  \label{table:machine_description}
\end{table}

%-----------------------------------------------------------------------------
% Milestone 1: CPU, Scheduling, and OS Services
%-----------------------------------------------------------------------------
%============================= section ==========================%
\section{CPU, Scheduling, and OS Services}

To accomplish following set of measurements, testing against a normally
running OS is insufficient. Therefore, we modified a few kernel settings so
that the test results are more accurate and closer to the theoretical values.

The machine we tested has two cores. We isolate one core by adding
\texttt{isolcpus=0} in bootloader configuration file. All the testing scripts
are run with \texttt{taskset -c 0\space./test} syntax. Therefore, we ensure minimum
number of system processes interfere with our testing code, thus minimizing
context switching overhead. Interrupts are also disabled during testing.
Finally, only one measurement is run at one time.

\subsection{Measurement Overhead}
The goal of this section is to report the overhead of reading system time and
the overhead of using loops to measure iterations of an operation.  These
measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
we used and modified the \texttt{cycle.h} package from FFTW\cite{FFTW}.  The
\texttt{cycle.h} is a superset of the rdtsc which optimizes according to
different machines.  To read the time stamp counter, use \texttt{getticks()}
function. To calculate the difference of two time stamp, use \texttt{elapsed}
function, which returns a double precision value.  To measure the overhead
time for reading time, we simply executed \texttt{getticks()} twice in a row
and took the difference. The idea behind this methodology was to start
tracking immediately before the the execution of the next command and stop
tracking as soon as the command is executed. The difference between the two
\texttt{getticks()} times will give us the number of ticks taken between the
two methods. Similarly, measuring \texttt{elapsed()} required that we add two
\texttt{getticks()}, one before and one after, the \texttt{elapsed()} method.
We then determined the difference between the two \texttt{getticks()} methods
that preceded and followed the \texttt{elapsed()} method. Taking that
difference, we subtracted the time taken for one \texttt{getticks()} method to
account for the time necessary to execute the succeeding \texttt{getticks()}
to arrive at our final result.

To measure the overhead time for using a single loop, we chose to measure
a \texttt{for} loop that will increment a variable from 0 to 9999, and
divided it by 10000 to obtain the overhead taken by a single iteration, which
includes incrementing and checking. In order to measure purely the looping
part of the \texttt{for} loop, we put the variable declaration before the
first \texttt{getticks()} method. This way, we measured the time taken by the
loop to check and increment the variable. We also took into account the time
taken by the \texttt{getticks()} method and subtracted it from our final time.
Our motive behind choosing to test the \texttt{for} loop comes from our own foresight
of using \texttt{for} loops plentifully in future tests.

Each test was ran 10000 iterations, the mean was taken.  Each experiment was
ran 100 times, and the average and standard deviation were taken.

In order to accurately translate "ticks" into human time, a preliminary
measurement of the correlation is made between ticks and number of cycles is
1 to 1.  Since each cycle takes 0.416 ns, given our CPU frequency, we can make
the correlation that 1 tick is equivalent to 0.416 ns.
\subsubsection{Prediction}
We originally predicted that, for \texttt{getticks()}, the operation should be
less than 20 instructions. Assume CPI is 1, it would take the hardware less
than 20 cycles to finish this instruction. And elapsed should take a little
more time to execute since it has subtraction, while \texttt{getticks()} is
simply reading data. Since the OS has to manage procedure call and returning
values, we think software overhead should be less than 5 cycles.

For \texttt{for()} loop, with one arithmetic operation, one comparison and one
jump, it should take about 3 cycles. The OS will have to initialize the loop
counter, which should be less than 2 cycles.

\subsubsection{Experiment Results}

The experimental results are presented in Table~\ref{table:rdtsc_overhead}.

\begin{table}
  \caption{Reading overhead performance (in cycles)}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation             & Hardware Est.         & Software Est.         & Prediction            & Mean          & Std. Deviation     \\ \hline
    \texttt{getticks}     & \textless20 (8.3 ns)  & \textless5 (2.1 ns)   & \textless25 (10.4 ns) & 45 (18.5 ns)  & $9*10^4 (3.6*10^4 ns)$    \\ \hline
    \texttt{elapsed}      & \textless25 (10.4 ns) & \textless5 (2.1 ns)   & \textless30 (12.5 ns) & 36 (14.4 ns)  & 0.5 (0.2 ns)    \\ \hline
    \texttt{for} loop     & 3 (1.2 ns)            & 2 (0.8 ns)            & 5 (2.0 ns)            & 7 (2.8 ns)    & 0.02 (0.008 ns)    \\ \hline
  \end{tabular}
  \label{table:rdtsc_overhead}
\end{table}
\subsubsection{Result Discussion}

The predicted performance was less than the measured performance. Our
predicted performance was based on the naive assumption that one task, such as
a jump, would only take 1 cycle to perform. However, looking at the measured
operation times, our assumption was proven to be false. This means that the
operating system and hardware were taking more cycles to perform each task.
A possible reason is that the compiler translate one line of code into several
instructions. Another reason might be CPU scheduling and context switching
between processes.

To note, both \texttt{getticks()} and \texttt{elapsed()} functions are
modified to be of inline attribute. Therefore, the measurement would eliminate
the overhead of procedual calls. On the other hand, the measured operation
time was close to the predicted operation time for the \texttt{for()} loop.
The difference here resided in our overlooking one or two operations while
making our prediction.

Our methodology centered around measuring the simplest possible operation for
reading time and for looping. Because we measured time immediately before and
immediately after the operation of interest, we successfully captured the
entirety of the operation. With this information, we will be able to use it as
a base from which we can accurately measure other performances.

\subsection{Procedure Call Overhead}
The goal of this section is to report the overhead of making a procedure call
with arguments. The number of arguments the procedure takes in will also be
taken into account in a series of tests. Each test will take in different
amounts of arguments ranging from 0 arguments to 7 arguments. These
measurements are important and necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a procedure call, we measured the time to make
a call to a procedure that takes in the parameters given and does not
processing thereafter. The procedure itself does not return anything either.
In order to take into account the different time costs as a result of varying
amounts of parameters, we made procedure calls with 0 to 7 parameters. Each
procedure call with different amounts of parameters was tested 1000, and the
average and standard deviation were taken. Each procedure call was precluded
and succeeded by two different \texttt{getticks()}. Because of this, we also
took into account the amount of overhead \texttt{getticks()} populated and
subtracted that from the total in each procedure call.

Each of the procedure call did no processing of its arguments nor did it have
a return statement. The decision to make the procedure call in this way was
motivated by our goal of obtaining purely the overhead of making the call to
a procedure. It was decided that return statements and any processing of data
would not be part of the procedure call itself, and thus not included as part
of the testing processes.

\subsubsection{Prediction}
The number of inputs and size of inputs should both affect the time it
take for a processor call. Without inputs, a procedure should simply be two
jump instructions. Thus, we predict that the hardware cost of procedure call
with no inputs should be about 2. The system needs to remember where the
processor is called so that it can return, which should only take 1 cycle. As
input size grow, more operations will need to be done to copy the
input.

\subsubsection{Experiment Results}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.6]{proc.eps}
  \caption{Procedure call performance (reading overhead included)}
  \label{fig:proc}
\end{figure}

\subsubsection{Result Discussion}
An interesting discovery is that, the relationship between number of
parameters and the time to issue a procedure call is not a strict linear
relation. This means that in one cycles, the system is able to execute more actions than one, which was what we had predicted.
It appears that, every 4 integer parameters added will add one extra cycle of
overhead of a procedure call. Thus, we can get to the conclusion that the os
is able to copy 128 bits of data at the same time.

Notice that we did not subtract the reading overhead in the graph, so the
actual overhead of procedure call should be 4~6 cycles. Thus, the overhead of
a procedure call is quite small. If the parameter size is large it will add to
the overhead, but still pretty fast.

\subsection{System Call Overhead}
The goal of this section is to report the overhead of making a system call and
compare it to the cost of making a procedure call. The system call with the
least overhead was chosen to accurately capture the time taken to make the the
switch from user mode to kernel mode. These measurements are important and
necessary for accuracy of future experiments.

\subsubsection{Experiment Methodology}
To measure the overhead time of a system call, we attempted to choose a method
with as little overhead as possible. Initially, our goal was to used
\texttt{getpid()} as our system call of choice. However, due to the potential
problem of having the \texttt{getpid()} system call getting cached and thus
not having an accurate overhead time of trapping into the kernel space, we
decided to choose a similar method, \texttt{getppid()}. In order to measure
the time of \texttt{getppid()}, we went about by simply adding
\texttt{getticks()} before and after the system call and calculated the
average difference and its standard deviation over 10000 runs. This method was
similar to the methodology described earlier in measuring loop overhead. We
then compared the results that we received to the results we measured earlier
for procedure calls.

\subsubsection{Prediction}

\subsubsection{Experiment Results}
The experimental results are presented in Table~\ref{table:systemcall_overhead}.
\begin{table}
  \begin{center}
    \caption{System Call Overhead (in cycles)}
    \begin{tabular}{|l|l|l|l|l|l|}
      \hline
      Operation   & Prediction            & Mean                       & Std. Deviation     \\ \hline
      getppid()   & 15 (7 ns)             & 242.35 (96.94 ns)          & 9.06 (3.624 ns)    \\ \hline
    \end{tabular}
  \end{center}
  \label{table:systemcall_overhead}
\end{table}

\subsubsection{Result Discussion}
In the process of running the experiments initially with getpid(), we noticed the effects of caching in skewing the measured results. This was something we hoped to avoid because the caching process would signify that trapping into the kernel will happen only during the first time the method is called. We went forth with getppid() because it proved to have much more consistent results in the initial and succeeding trials. The inaccuracy found in our predictions can largely be related to our lack of knowledge regarding the entire set of steps taken when trapping into the kernel. There can also be some extra processing needed in retrieving the parent id, such as verification, that we naively did not acknowledge. In comparison to procedure calls, system calls take almost 4 times the amount of time. This is due largely to the switching into kernel space and all processing that happens thereafter, such as validation, before the operation can go on.

\subsection{Task Creation Time}
The goal of this section is to report the time taken to create and run both
a process and a kernel thread, and to compare the two time costs.

\subsubsection{Experiment Methodology}
In order to measure time taken to create and run both a process and a kernel
thread, we used the basic principle of precluding and succeeding the target
section of code as we have prior. In order to create a kernel thread, we used
the method \texttt{pthread\_create()} to create and run a thread.

\texttt{pthread} allows us to avoid having to write kernel code and this
flexibility was one of the reasons why we decided to use it. The other reason
is that the thread is created executing any argument given as part of its
parameter. This means that we will not need to explicitly call any end
operation on the thread.

The arguments given to the \texttt{pthread\_create()} method were defaulted as
to avoid overhead that is separate from the task creation and minimal amount
of running. This test was repeated 10000 times, and the average and standard
deviation were taken while taking into consideration the overhead of reading
time. We used the same procedures for processes. We created processes using
\texttt{fork()} and terminated each process using \texttt{\_exit()}
immediately after creation and running.  The times were taken immediately before creation and another time was taken immediately after the process returns from \texttt{\_exit()}.Each experiment has 10000 iterations,
we run 100 experiments for process creation, and 1000 for thread
creation(since process creation is too slow) and the average and standard
deviation were taken while taking into consideration the overhead of reading
time.

\subsubsection{Prediction}
The time needed to create a task is really difficult to predict, since it
really dependents on how OS define a task, which means details such as process
data structure, thread data structure, implementation algorithms, will all
affect the time. Since we have little knowledge of linux kernel, we don't
think we can make a reasonable guess. One thing we would expect is that, the
time to create a process should be longer than creating a thread, since
processes are more heavy weighted than thread.

\subsubsection{Experiment Results}
\begin{table}[h]
  \begin{center}
    \caption{Thread creation performance (reading time included)}
    \begin{tabular}{|l|l|l|}
      \hline
      operation            & Mean of experiments       & Standard deviation       \\ \hline
      create process       & 209194.13(87.02 microsec) & 28245.38(11.75 microsec) \\ \hline
      create kernel thread & 3030.83(1.25 microsec)    & 68.57(28.53 ns)          \\ \hline
    \end{tabular}
    \label{table:thread_creating_results}
  \end{center}
\end{table}


\subsubsection{Result Discussion}
From the result, we could observe a significant difference between the time it
took to create a process and a kernel thread. We believe it is because
processes have much more complicated structure than threads. For example,
threads share memory while processes have separate address spaces. Such
a difference is reasonable in the sense that, threads are the minimum unit of
scheduling, and processes take advantage of threads to get better performance.
One more thing to mention, we actually measured the time it take for a kernel
module to create a kthread, and it seems to be even more time consuming than
process creation. We are wondering why would that happen, since a kernel
module is already in kernel level, it won't need to cross the boundary of user
level and kernel level again, why would it take so much time? Not to mention
it is a thread not a process.

\subsection{Context Switch Time}
The goal of this section is to report the time taken to context switch from
one process to another in comparison to the time taken to context switch from
one kernel thread to another.  A context switch in essence is the process of
storing current process state and restoring another. More specifically,
storing a state means changing the process's state to ready or blocked;
restoring a state brings the process from ready to running.

The overhead time spent in a context switch consists of several parts.  First
and foremost, by definition time needs to be spent saving and restoring
a process's states.  Secondly, process caching may also influence the overhead
time. Thirdly, virtual memory mapping, synchronization of memory caches,
paging, these elements also may not be ignored when switching between
processes. Finally, interrupts may occur in the middle of measurement.

In this paper, we mainly compare a process-level context switch and a kernel
thread-level context switch.

\subsubsection{Experiment Methodology}
In order to measure the time taken to make a context switch between two
processes, we utilized blocking pipes to ensure synchronization. The way we
designed the test to measure just the time for context switching required the
use of the pipe's ability to pass data.

At first, we tried to use a simple \texttt{fork} followed by and \texttt{exit}
in the child process, and use \texttt{waitpid} in the parent process in order
to achieve synchronization. The implementation of this method is trivial, but
it turned out to generate irregular results this methodology was abandoned. Forking child processes within
a loop easily drains memory, while using \texttt{waitpid} adds a unstable
overhead to the measurement.

For user-level process context switching, we eventually decided to use a pipe
mechanism to ensure synchronization. We first spawned a child process
, wherein a time was taken before the fork() call, and a \texttt{write()} procedure was done in conjunction with a pipe with which the time into the pipe as part of its arguments. The child process
immediately calls exit after and a context switch is made back to the parent
process.  Within the parent process, we immediately call a \texttt{read()}
procedure and store the first time taken in the child process.  A second time
is taken right after and the difference is taken. In the parent process, we
used \texttt{waitpid()} to ensure the following order of execution with the correct parent and chil: parent, child,
parent. Because pipes forces context switch into the kernel, the significant
overhead of the pipe's \texttt{write()} and \texttt{read()} operations were also
measured and taken into account in the final calculations since they are essentially not part of what we are attempting to benchmark. This overhead was benchmarked by performing multiple \texttt{write()} and \texttt{read()} operations with pipes, and an average was taken over the number of times this was performed. Each context switch
was performed 10000 times and an average was taken. This measurement tool was
ran 10000, and an average and standard deviation was taken for the final
results.

For kernel-level thread context switching, we also used pipe to force two
threads to synchronize.  First, we created two pipes p1 and p2. In both of
pipes, define the first integer as reading, and the second integer is writing.
Then, we use \texttt{pthread\_create()} and \texttt{pthread\_join()}to create
two threads: Ta and Tb. Set the pthread scope
\texttt{PTHREAD\_SCOPE\_SYSTEM}to ensure that this thread is a kernel thread.
After this, we timestamp the initial time t0 in the start of thread Ta.  Let
both threads communicate to each other though the two given pipes. Ta writes
data to p1 and listens to p2; Tb writes data to p2 and listens to p1.  The
communication is repeated 1000 times in both processes.  In the end of Ta, get
time stamp t1. The time duration $dt = t1-t0$ includes 2000 context switches,
so we can get the average context switch overhead by $dt/2000$. This precess
is repeated
100 times.

\subsubsection{Prediction}

The prediction for context switching is not easy. But we can be sure that
a kernel-level thread context switching would spend way less time than
a process one.

For process context switching, a process must call \texttt{wait}, jump into
kernel mode, then the kernel picks the right process to wake it up, pass
parameters. If the process is switched out to memory, hardware may also add
extra overhead.
For kernel threads, since threads share the same file descriptors, signals,
etc.\ , we estimate a smaller hardware overhead.


\subsubsection{Experiment Results}

The results can be found in table~\ref{table:contextSwitch_overhead}.

\begin{table}
  \begin{center}
    \caption{Context switching overhead performance (in cycles)}
    \begin{tabular}{|l|l|l|l|l|l|}
      \hline
      Operation          & Hardware Est.   & Software Est.         & Prediction            & Mean          & Std. Deviation     \\ \hline
      Process            & \textless50000  & \textless20000   & \textless60000 & 71582.43 (0.0286 ms)  & 507.32 (0.001 ms)    \\ \hline
      Kernel Thread      & \textless10000  & \textless10000 & \textless 20000 & 23425.98 (9760.8 ns) & 211.34 (88.058 ns)  \\ \hline
    \end{tabular}
    \label{table:contextSwitch_overhead}
  \end{center}
\end{table}

The results are pretty much similar to the predictions.

\subsubsection{Result Discussion}

Forcing a context switch is not simple. When measuring process-level context
swiching, at first we only used forking to test the overhead, but the results
were very irradical.

During implementation of thread-level context switching, we first tried using
a mutex and wait-and-signal mechanism to preserve synchronization. However,
This method leads to complex structures and to the necessity to test overheads
of using semaphores.  Using the pipe mechanism forces the context switch
orders without much complexity, and it's more trackable to debug.


%-----------------------------------------------------------------------------
% Milestone 2: Memory
%-----------------------------------------------------------------------------

%============================= section ==========================%
\section{Memory}
\subsection{RAM access time}
\subsubsection{Experiment Methodology}
Our methodology was planned based on the experiments that were done in
the lmbench paper. In order to measure the time required to access the
two caches and main memory with the least amount of overhead, we chose to
do memory accesses using integer arrays. We created arrays of different
sizes ranging from KB to 1GB with different strides ranging from 512B to
32KB\@. With each combination of array size and stride length, we performed
1000000 integer accesses just as the lmbench paper did. While they performed
p = *p; in their paper, we did the C++ respective version. Some of the
problems that we encountered in our creation of the methodology included
pre-fetching. In order to bypass this behavior, we followed the lmbench
paper and performed random accesses to the array. In order to bypass the
possibility of our random number generator generating the same number, we
preloaded integers within the array itself that acts as the location of the
next integer access in the array that also takes into account the stride.
When running the code, the initial location is randomly accessed. The time
of these 1,000,000 accesses were then averaged and given as the time of
access for that particular combination of array size and stride length. All
time calculations took into account the overhead times involved. The entire
experiment was done 10 times, and the average is show in the results below.

\subsubsection{Prediction}
From previous experiments, we noticed how difficult it was to estimate the exact numbers for time of access. Therefore, our prediction for this part will be done on what we believe the graph will look like. Based on the fact that accessing the L1 cache is several times faster than accessing the L2 cache, we believe that there will be at least two plateaus and two sharp inclines on the graph. The inclines represent the increase in time necessary to access the L2 cache and the main memory. Based on the hardware specifications, these two inclines should be at the 32KB and 3072KB mark. The plateaus will be a result of the time to access memory from within the cache. Because we are dealing with larger amounts of memory in larger quantities, the numbers were also predicted to be much larger than what we have seen in previous experiments. We also expect a small amount of latency required to access elements in arrays of sizes smaller than the L1 cache. We expect to be about 4 cycle's time or about 1.6 ns.

\subsubsection{Experiment Results}
\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{cahce_access.png}
  \caption{Latency for individual integer accesses to main memory and the L1 and L2 caches}
  \label{fig:cache_access_time}
\end{figure}

\subsubsection{Result Discussion}
In the results graph, we can see a clear staircase shape similar to what we have expected. We see that accessing memory less than the size of the L1 cache (from 10 to 15 on the graph's x-axis) is constant. The increase in time at 32KB shows that we are now accessing memory outside of what the L1 cache can hold. The relative plateau formation from 32KB to 3072KB (from 15 to 21 on the graph's x-axis) shows the latency of accessing L2 cache. Then there is an extreme incline in latency thereafter which represents the difference in time needed to access the main memory compared to the L2 cache.

An unexpected result was the small dip found in the 128KB region. This may be
a result of the possible situation where the stride is not large enough in
comparison to the array size. This would mean that the next integer to access
would be within some bytes range that was already cached. We can see these
smaller dips prevalent throughout the entire graph in different place and in
varying degrees. Therefore, for the analysis of our methodology, our method
was not completely perfect and there were situation as such that we did not
consider. However, for the most part, the methodology is correct. The result
shows the trend that we have expected and also matches with what McVoy et al.\
reported in the lmbench paper.


\subsection{RAM bandwidth}
\subsubsection{Experiment Methodology}
In order to measure the bandwidth of memory, we allocate different size of array in memory, then read and write the whole array with a stride of 64 bytes, which is our cache line size, and measure how fast we can read. \\
On this particular measurement, we implemented our experiment both in c and assembly. We implemented repeating memory read and write as assembly functions, and used c as a framework to piece the assembly functions together to record the data. We used nasm as the assembler, with the following compiling command:\\

nasm -f elf <source.asm> -o <target.o> \\
gcc -funroll-loops -m32 -mtune=pentium <target.o> <source.c> -g -lm -o <target>\\

We have the assembly function read the array 100 times consecutively, then free the array and recall the assembly function. We repeatedly call the assembly function 100 times. Thus, we read the array for 10000 times in one experiment. We output the mean time it takes to read the array. Then we repeat the experiment 10 times, get a stable mean value and standard deviation of these experiments. In this process, we've already subtracted the getticks overhead from our experiment result. Then we did the same experiment for memory write operation.\\
In order to avoid our read being cached, we used the Intel SSE4 streaming load assembly operation ``movntdqa'', which claims to read 16 bytes of data in one operation, skipping caching hierarchy.\\

\subsubsection{Prediction}
According to our hardware specification, our maximum memory bandwidth should be:\\
$$ 667000000 * 2 * 64 / 8 = 10.672GB/s $$
Thus, we predict that our memory read bandwidth will be less than this value.\\
However, in case where cache is used, this will not be a good guess. With cache, the bandwidth can be much much higher than the theoretical memory bandwidth, since cache access speed can be orders of magnitude faster than main memory. We chose SSE4 with that in mind, and try to avoid it.\\
We also predict that memory write bandwidth would be slower than read bandwidth, since there are more extra work to do for operating system than reading.\\

\subsubsection{Experiment Results}
Our results are shown in Figure~\ref{fig:ram_bw}, and Table~\ref{table:mem_bw} \\

\begin{table}
  \begin{center}
    \caption{Memory read bandwidth(100 experiments)(note that the bandwidth larger than theoretical maximum will be explained in next section)}
    \begin{tabular}{|l|l|l|l|l|}
      \hline
      Read size   & Read Bandwidth(MB/s) & Read Std deviation & Write Bandwidth(MB/s) & Write Std deviation \\ \hline
      128B        & 82877.6275931        & 138.756108952      & 63510.9140232         & 21332.758409        \\ \hline
      256B        & 58203.8645379        & 18.077260397       & 83370.3221002         & 16688.8326443       \\ \hline
      512B        & 84680.7004147        & 37.1344832832      & 91015.9836298         & 8204.6841546        \\ \hline
      1KB         & 118596.488159        & 4448.09137085      & 90154.1040324         & 10747.1791029       \\ \hline
      2KB         & 131887.820751        & 125.361002649      & 96791.8864987         & 34.141795185        \\ \hline
      4KB         & 127782.200151        & 13634.5440191      & 93483.5398672         & 7359.43759536       \\ \hline
      8KB         & 141376.726491        & 2376.56603179      & 96269.3441776         & 1723.81340114       \\ \hline
      16KB        & 135623.658387        & 1657.95149951      & 129375.416649         & 3917.06137859       \\ \hline
      32KB        & 103241.630457        & 1749.79398914      & 106635.517589         & 1664.03160626       \\ \hline
      64KB        & 29339.5731386        & 252.757947951      & 14794.8984809         & 42.9056993923       \\ \hline
      128KB       & 29605.2150849        & 97.1859818669      & 15055.5027172         & 49.7087655775       \\ \hline
      256KB       & 29889.2887814        & 87.4971284782      & 15083.895086          & 63.2402781397       \\ \hline
      512KB       & 29994.1560773        & 88.8630251946      & 15085.1267137         & 44.5793993099       \\ \hline
      1MB         & 29620.6738502        & 1003.58594801      & 15076.8142427         & 43.0446913139       \\ \hline
      2MB         & 22766.1236277        & 2737.16050128      & 12682.8443067         & 1861.40116882       \\ \hline
      4MB         & 6289.0487096         & 82.0998898472      & 2465.6766035          & 107.355250748       \\ \hline
      8MB         & 5432.4525749         & 149.896863299      & 2032.3571343          & 8.47702555378       \\ \hline
      16MB        & 5478.9018851         & 24.9918155661      & 2022.4700689          & 8.38293586944       \\ \hline
      32MB        & 5488.0249203         & 18.6041543511      & 2017.1904064          & 8.07114584736       \\ \hline
      64MB        & 5473.0265441         & 29.7586307672      & 2012.1484931          & 10.3703258606       \\ \hline
    \end{tabular}
    \label{table:mem_bw}
  \end{center}
\end{table}


\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.4]{RAM_bw.eps}
  \caption{Memory Read and Write bandwidth}
  \label{fig:ram_bw}
\end{figure}

\subsubsection{Result Discussion}
First of all, from the graph we can clearly see that it is obvious ``movntdqa'' operation did not bypass cache at all. The drop in 32KB size and 3MB size are due to the L1 cache and L2 cache which has exactly the corresponding size. Despite the fact we did not by pass cache, we still believe the memory bandwidth we measured when reading data that has a size over 4MB is a reasonably good approximate of the actual memory bandwidth, which is about 5.5GB/s.\\
There are several reason that this value is a reasonable approximate: First, since the array size is large enough, every time we repeat the reading the array, the array data will not be in cache any more, cpu will need to get them from memory anyway. Second, although we are aware that cpu will read one whole cache line(64bytes) at a time, we think that, the time of 1 read 64 bytes(which is essentially one ``movntdqa'') from memory followed by 3 ``movntdqa'' 16 bytes read from cache, will not be significantly longer than 1 read of 64 bytes from memory, since cache access speed is significantly faster than memory access.\\
One other overhead we did not take into consideration is virtual address translation time. We did not figure a good we to measure and subtract that from our result. We believe that would be the main cause of error in our result.\\
The other interesting fact is how memory write is slower than memory read. The hardware of cache and memory should not perform a significant different speed between read and write. Thus, we believe this phenomenon is caused by operating system overhead. When writing, the lock mechanism and extra book keeping work is more complicated than mere read, thus caused the bandwidth degradation.\\
It is reasonable to see how standard deviation bump up when cache boundary is reached. Also, the bump of standard deviation when array size is small indicate that, when reading very small data, the function is more vulnerable to disturbance of other processes and cpu scheduling in the system.\\

\subsection{Page fault service time}
A page fault occurs when the OS tries to fetch a page that was swapped out from the physical memory, and thereby it needs a further fetch from the disk. Measuring the page fault service time means measuring the time interval between a page fault occurance and loading the page from the disk.
\subsubsection{Experiment Methodology}
One neat way to conduct the measurement is to use mmap (memory mapped files). The mmap function is a system call that directly maps a file to the virtual memory space of the calling process.Upon mapping, the actual data won\'t be copied into main memory until accessed by the process. This is because the mmap approach is using lazy loading, which makes it perfect for our measurement of a page fault service time.

The experiment is devided into two steps; first part is to prove that a page
fault occurces when sequentially accessing the file; second part is to
calculate the average service time during a page fault.  Both of the
experiments are run against a dummy file we created with the same repeated
letters, size 400KB\@. The page size of the system is 4KB, which is got from a
sysconf system call.

In the first experiment, since the mmap returns a pointer to the starting address where the file is mapped in the virtual memory, we iterate the pointer through all letters in the file, that is we are accessing one byte at a time. Since the file is loaded into the physical memory page by page, this would force a page fault accessing the file every 4KB (size of one page in the system). We decide to do a backward scan on moving the pointer in case of a potential read-ahead.

In the second experiment, we set the scan stride to be 4KB\@. This forces a page
fault every byte of access in that mmap loads the file from disk to physical
memory page by page, thus gives us a measurable results to calculate the
average.
We iterate 10,000 times mmap-ing and munmap-ing the file, and access a byte every 4KB to the file. Since the size of the file is 400KB, this gives us 1,000,000 samples.
One thing to notice is that to repeat the measurement,  the file may be kept in the file system buffer cache. Therefore, the buffer cache should be flushed out during each iteration to get an accurate result.

\subsubsection{Prediction}
Each page fault implies loading 4KB from the disk, which should spend much longer than directlly accessing physical memory. We estimate this process to around This should take from 8 to 10 ms on the bare hardware. The software overhead may add 1ms.
See table~\ref{table:pagefault} for data.

\subsubsection{Experiment Results}


\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.45]{pagefault_sequential.png}
  \caption{Sequential Access mmap file}
  \label{fig:pagefault_sequential}
\end{figure}

\begin{table}
  \begin{center}
    \caption{Context switching overhead performance (in cycles)}
    \begin{tabular}{|l|l|l|l|l|l|}
      \hline
      Operation             & Hardware Est.         & Software Est.         & Prediction            & Mean        & Std. Deviation       \\ \hline
      mmap file accessing   & 10ms                  & 1ms                   & 11ms                  & 7.4ms       & 1.1 ms               \\ \hline
    \end{tabular}
    \label{table:pagefault}
  \end{center}
\end{table}

See table~\ref{fig:pagefault_sequential} for first part of the experiment.\\
See table~\ref{table:pagefault} for second part of the experiment.


\subsubsection{Result Discussion}

Dividing the result by the size of a page, which is 4KB, we get an average of 1.9ms accessing a single byte by a page fault. This is 40,000 times as big as the The latency of accessing a byte from main memory.

%============================= section ==========================%
\section{Network}
%----------------------------- subsection --------------------------%
\subsection{Round Trip Time}
\subsubsection{Experiment Methodology}
Round trip time measures the time spend for a established tcp connection to send one packet and receive an acknowledgement of that packet. AKA, the time for the packet to do a "round trip".\\
In order to measure it, we used c language server-client flavored socket program to establish connection, transmit and measure time. We measure the time it took on a established connection for one single write of one byte of data, and one single read of one byte of data implemented using the write() and read() function on an established socket connection. We measured one packet roundtrip time on a established link for 100/1000/10000/100000 times to observer different roundtrip time. Then we break that connection, repeat the experiment for 100/10 times, to get a mean and standard deviation. The number of experiments we did is based on the stability of the measured data. When standard deviation is small enough, we consider the data is stable enough. Note that, we do not want to consider TCP handshake time in RTT, we did not include the connection overhead in the measurement. Also, measuring multiple transmission on one established connection will help us amortize and reduce any setup and teardown overhead of the connection.\\
Since round trip time depends heavily on the internet connection environment between the server and client, we will describe our network environment in the next part. \\
In terms of ICMP, we also used c socket to perform ping operation. We measure the time between sending and receiving the ICMP packets, repeat this for 1000 times for each experiment, and repeat the experiment for 100 times just like the roundtrip experiment. \\
We also tested roundtrip time under different server queue size, some interesting results are found.\\
\subsubsection{Experiment Setup and Prediction}
Since complex network routing will significantly affect the stability of experiment, we used direct cable connection for our experiment. This will avoid any disturbance of out side network traffic. \\
The specification of the other machine which is used as server is give in form \\ %TODO \ref{fig:server}add the spec form here
When measuring roundtrip time(RTT), we expect RRT would be much smaller when measuring loopback interface than measuring remote interface, since the operation involved in a packet transfer locally is essentially memory copy operation, which we would expect would be at least one order of magnitude faster than actually transfering packet in the network. When measuring ping latency, we expect the same behavior. We also expect that pinging loopback interface would be at least one order of magnitude faster than pinging remote interface. \\
When comparing RRT and ping latency, we predict that RRT measured through TCP
would be slightly slower than ping, since ICMP is stateless, there should be
less overhead than TCP RRT\@. We expect the performance different be within one
order of magnitude.\\
\subsubsection{Experiment Results}

\begin{table}
  \begin{center}
    \caption{Comparison of Roundtrip time between TCP and ICMP}
    \begin{tabular}{|l|l|l|}
      \hline
      Protocol  & loopback(ms)     & remote(ms)              \\ \hline
      ICMP ping & 0.00918/0.000307 & 0.227/0.00919           \\ \hline
      TCP RRT   & 0.0462/0.00556   & 0.534/0.0235(min 0.270) \\ \hline
    \end{tabular}
    \label{table:roundtrip}
  \end{center}
\end{table}

Table~\ref{table:roundtrip} shows the mean time it took for a single ping
request and TCP RTT with standard deviation. In In this table, ICMP was tested
1000 times for each of the 100 experiments, and TCP RTT was tested 100000 times
for each of the 10 experiments. The ICMP ping seems to be very stable, we did
not further test other experiment settings. However, TCP RTT is not as stable
as ICMP ping. We tested a few different experiment settings, the data in this
table is the most stable one we got. Our original data can be found in the
appendix. We will discuss this phenomenon in the next section. \\
In addition to the data we showed in table~\ref{table:roundtrip}, we also
observed a min round trip time of about 0.270ms and max round trip time of
about 0.794ms. In this case, we actually prefer to see the min value as the
actual round trip time, since the transmission time on wire is not avoidable.
Based on that, any other disturbance should be rule out. Not to mention our
original data is not very stable.\\

\subsubsection{Result Discussion}
The experiment results matched most of our predictions. TCP RTT performs one order of magnitude slower on direct connection cable than loopback interface. ICMP ping time increase in direct cable connection more than one order of magnitude, which is a little more than we expected, but is actually reasonable. When executed on loopback interface, the actual transmit time is only a small portion of the overhead. However, when executed on a remote connection, the time to transmit and read the data from NIC become critical. That is why the difference between TCP RTT and ICMP ping on direct cable connection is much less than on loopback interface. Actual transmit time dominated the whole processing time.\\
From the result that ICMP ping is much faster than TCP RRT on loopback interface we confirmed that TCP has a much more complicated work flow.\\
There is an interesting phenomenon we observed during experiment we want to mention. When measuring TCP RTT, the result can be very unstable, and the reason is unknown. We have attached our original data in the appendix to show the phenomenon. We observed almost the same mean RTT but different standard deviation for different experiments. In experiment 2 and experiment 3, we both used server queue size of 100, while in experiment 1, we used server queue size 5. From these data we may say that a larger queue size decreases the magnitude of fluctuation in our experiments, but when comparing experiment 2 and three, we prefer to believe that there is some other factors which affects the stability of TCP RTT more. \\

%----------------------------- subsection --------------------------%
\subsection{Peak Bandwidth}
\subsubsection{Experiment Methodology}
In order to measure peak bandwidth, we open a TCP connection, and keep sending data through it. We measured the time to send 1MB/10MB/100MB/1000MB/2000MB of data, 256 bytes per packets, to get the stable peak bandwidth. We do some experiments both on loopback interface and direct cable connection, some more is done on loopback interface to explore the stable performance. The data packets are sent one way from client to server, server will simply discard anything it receives, and will automatically stop when client send a package with less than 256 bytes of data. Client is the machine we are measuring, and the specification of server machine can be found in \\%TODO \ref{fig:server}add the spec form here
We chose 256 bytes as one data packet also to avoid being split into multiple packets, 256 bytes is within the 1500bytes MTU.\\
Our initial experiments are all done using queue size of 5 on server, we then realized that queue size may significantly affect performance, so we added more experiments with various of queue size to exploit the actual peak bandwidth.\\

\subsubsection{Setup and Prediction}
Like the RTT experiment, we used a direct cable connection to measure peak bandwidth. Both of our server and client machine has a ethernet interface labeled 10/100/1000Mb. We used a twisted pair cable, which is a 100BASE-TX standard cable, with RJ45. With this setting, the peak bandwidth we can reach would be bounded by the cable we used. We did not expect the system to fully utilize the bandwidth the cable can provide, we think 80 percent would be a fair guess. Thus, we predict our peak bandwidth to be about 10MB(80Mb).\\ % TODO Is server machine also labeled 10/100/1000Mb? check!
For loopback interface, we expect the bandwidth to be much higher, since it is bounded by the speed of memory operation. Loopback interface is actually a buffer treated like file in memory, thus sending data to loopback interface is essentially copying memory data. Because we don't know how exactly such operations are performed, we are not able to make any specific guess. Though we know such kind of copying is much slower than mere ram read bandwidth, which we measured in previous sections. So, we predict loopback interface peak bandwidth to be smaller than mem ram bandwidth, but should be within 1 order of magnitude. \\
\subsubsection{Experiment Results}

Figure~\ref{table:peak_bandwidth} shows our experiment result in MB/s. Each experiment for cable connection and loopback interface with queue size of 5 is repeated 10 times to get the mean value and standard deviation. We did not measure bandwidth when transferring 1000MB remotely, because we think the result we get from transferring 10MB and 100MB are already stable enough. Also, it is too time consuming to repeat such an experiment to get a stable result.\\
We did loopback experiments with queue size of 100 after we finished the previous experiments. We realized that queue size may affect performance significantly, so we added these experiments. Each of these experiments is reprepeated 100 times to get stable mean and standard deviation.\\

\begin{table}
  \begin{center}
    \caption{Peak bandwidth in MB/s}
    \begin{tabular}{|l|l|l|l|l|l|l|}
      \hline
      Amount of data sent      & 1MB        & 10MB       & 100MB       & 1000MB    & 2000MB   \\ \hline
      Remote                   & 12.3/0.463 & 11.4/0.134 & 11.2/0.0122 & -         & -        \\ \hline
      Loopback(queue size 5)   & 45.9/3.11  & 65.3/3.19  & 67.9/3.10   & 68.7/1.73 & -        \\ \hline
      Loopback(queue size 100) & 51.7/14.4  & 129/27.7   & 180/5.30    & 191/3.47  & 191/4.84 \\ \hline
    \end{tabular}
    \label{table:peak_bandwidth}
  \end{center}
\end{table}

\subsubsection{Result Discussion}
Remote peak bandwidth matched our prediction pretty good. While the cable support 100Mb/s(12.5MB/s) speed, our result showed 11.2MB/s speed, which is about 90 percent of the capability of the cable. We think that is a very good performance. If a better connection is used between the server and client, we believe a much higher bandwidth could be supported. \\
For loopback interface, in our first set of experiments where server has queue size of 5, we got peak bandwidth 68.7MB/s(549.8Mb/s). We think this is bandwidth is way too small. Then we realized the size of queue size matters. We redo the experiments with various queue size, and we found when queue size is bigger than some threshold, there is no longer bandwidth increase. We did not intend to find that exact queue size, so here we choose server queue with size 100 for convenience. With increased queue size, we see an increase in peak bandwidth too, and stabilizes at 191MB/s(1448Mb/s). Although this is still significantly slower than memory read bandwidth, we believe it is a reasonable value, because the operations included is much more complicated than mere memory reads. \\

%----------------------------- subsection --------------------------%
\subsection{Connection Overhead}
\subsubsection{Experiment Methodology}
To measure connection overhead of TCP protocol, we did not count the time to obtain a local socket as the overhead, we only measured the time used to establish and close the connection. We measured remote TCP overhead for 100 times in one experiment, and repeated the experiment for 100 times to get mean and standard deviation. For loopback interface, we measured TCP overhead 1000 times in one experiment and repeated the experiment for 100 times to get mean and standard deviation. \\
We set the server to block on read after connection is established, so that it is the client who actually issued a close to the connection. We did not send any data through the connection, because TCP may send such data together with TCP SYN/ACK.\\

\subsubsection{Setup and Prediction}
Like the previous experiments, we used direct cable connection between the server and client machine.\\
TCP connection establishment need 3 handshakes, so we expect to observe a connection overhead of at about 1 RTT for remote connection. For local connection, since round trip time is just memory operation, we expect it to be a little bit longer than local RTT time, since there will be more data structure initialization than simply sending data.\\
When closing a TCP connection, the client should send a FIN to server and wait
for FIN from server to actually close the connection, then send an ACK to that
FIN\@. So, the closing of a TCP connection should cost 1 RTT too. However, we
think the operating system may chose to do this asynchronously, so that we
cannot actually measure this closing behavior.\\

\subsubsection{Experiment Results}

\begin{table}
  \begin{center}
    \caption{TCP connection overhead}
    \begin{tabular}{|l|l|l|}
      \hline
      Amount of data sent & open           & close            \\ \hline
      Loopback            & 0.0486/0.00836 & 0.0350/0.00481   \\ \hline
      Remote              & 0.252/0.00419  & 0.00702/0.000301 \\ \hline
    \end{tabular}
    \label{table:tcp_overhead}
  \end{center}
\end{table}

Table~\ref{table:tcp_overhead} showed the overhead for establishing and destroying a TCP connection. Each experiment include 100 times to open and close a TCP connection, each experiment is repeated for 100 times.\\

\subsubsection{Result Discussion}
The result roughly matched our prediction very well. But one problem is that,
from our previous experiments, we have a measured TCP RTT of about 0.50ms.
While here, our TCP connection open overhead is just 0.25ms, which is half of
TCP RTT\@. This is abnormal because the client have to wait at least for one
round trip time to receive the SYN signal from server. After that, the client
can send ACK asynchronously. Like what we have explained in the round trip
section, we prefer to choose the minimum round trip time that we measured as
the actual round trip time. If that is the case, the connection overhead we
measured here matched the TCP handshake behavior.\\
In terms of close operation, since linux treat loopback interface as a file, when closing connection with close, it is more likely to just close the file descriptor. Then the operating system handles the actual work of destroying the connection with the server. Quoting from Gnu c library manual online\cite{GNU}, ``If there is still data waiting to be transmitted over the connection, normally close tries to complete this transmission.''. Although it is said so in the manual, we observed some behavior which may indicate that the client did not actually tries to negotiate the closing of the connection with server at all. We found sometimes establishing a connection on the same port number after a previous experiment finished is not allowed by the system, saying that ``binding to socket failed: Address already in use''. In conclusion, no matter the operating system negotiated the closing of the connection with the server or not, it should have done it asynchronously, so that we cannot measure that behavior in our experiment. \\

%============================= section ==========================%
\section{File System}

%----------------------------- subsection --------------------------%
\subsection{Size of cache}
\subsubsection{Experiment Methodology}
In order to estimate the size of the file cache, we created a variety of files
with sizes ranging from 50MB to 2GB\@. With each different file, we measured the
time it took to read the file over an average of 10 times per file. This idea
behind the methodology comes from the idea that attempting to read within a
file that fits within a cache will have relatively the same read time. However,
as soon as we have a file whose size is larger than the cache size and we try
to access a part of the file outside of the cache, the read time will be much
larger. Therefore, we attempted to read from a variety of file sizes in order
to capture the size at which the large read time occurs. The large jump in read
time will indicate to us the size of the file cache of the system.

\subsubsection{Prediction}
We can safely say that the size of the cache will be smaller than main memory,
which is 2GB\@. Since the OS will take a large portion of the main memory for
file system cache, we predict that the file cache size will be less than half
of the size of main memory, <1GB.

\subsubsection{Experiment Results}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.6]{file_cache_size.png}
  \caption{Left x-axis is the size of file and y-axis is average read time. Both are on log scale.}
  \label{fig:file_cache_size}
\end{figure}

The results can be found in figure~\ref{fig:file_cache_size}.

\subsubsection{Result Discussion}
The results show that there is a small increase in read time at 200KB\@. From
there, we have a very small increase in read time until 776MB
(2\textsuperscript{19.6} KB). The read time then begins to level out again
towards the end after 1.5GB (2\textsuperscript{20.6} KB). This indicates to us
that the file cache size is a little larger than 776MB\@. We are able to deduce
this from the relationship between file cache hits and the size of the file. If
the file is able to fit within the cache, then we will be reading from caches
and thus have a relatively stable reading time for all files of sizes that fit
within the cache. Those files that are too large to fit in the cache, however,
will cause cache misses and thus longer time to fetch from outside of the
cache. This increase in time can be seen at around 776MB, which indicates to us
that this is the relative size of the file cache.

Overall, our methodology was successful in helping us attain the information we required. However, to pinpoint the size of the cache more clearly, future experiments will benefit from decreasing the rate in which we are increasing the file sizes. As vague as our estimates may have been, our estimates were accurate relative to the information attained from the experiment. The experiment does, however, give us a fairly loose range from which we can expect our cache size to be.

\subsection{File read time}

\subsubsection{Experiment Methodology}
The basis of our experiment is to perform both sequential and random reads from files of different sizes, which we made to range from 8KB to 200MG\@. The purpose of this experiment was to compare and contrast the read time for sequential and random reads. In order to do so, we needed to use a raw device to ensure that we were not measuring cached data. As a point of clarification, a raw device is a special block device file that bypasses the OS's caches and buffers. However, raw devices were deprecated on Linux systems, therefore we alternatively used the \texttt{O\_DIRECT} and \texttt{O\_SYNC} flags associated with the \texttt{open()} system call. The \texttt{O\_DIRECT} flag ensures that the file is not brought into the file buffer cache, and every read goes to the disk. The \texttt{O\_SYNC} flag ensures that the reads were performed synchronously.
We also verified that the file is not brought into the cache by observing the output of the \texttt{vmstat} command in linux. For reading sequential blocks, we simply used read() and put time markers for benchmarking around the read commands for reading an entire file. The random reads were harder to perform because we had to indicate where to fetch the next block. In order to do so, we used lseek() along with read(). The overall methodology for benchmarking random file accesses was the same as sequential accesses except for the need to use lseek() to indicate the amount to bytes to offset before starting the read. All reads for different file sizes were done 100 times and the average was taken per block.

\subsubsection{Prediction}
We expected the time for random read to be much larger than the time for sequential read. Sequential reads can benefit from locality and pre-fetching to a greater extent than random reads will be able to. Therefore unless the order in which random reads were made was almost the same as sequential reads, then random reads should be significantly larger than sequential reads. We also suspect that the read time for sequential reads to have a downward trend at some point as a result of benefiting from reading blocks that are sequential. We predicted that the hardware would take about 6600 ns, which is the hardware's average latency and track to track seek time. For random access time, we added the average seek read time to this for a total of 20600 ns of estimated hardware time.The software for both was simply an estimated time that it would take for the command to propagate to the hardware.

\subsubsection{Experiment Results}

The results can be found in figure~\ref{fig:file_read_times}.

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.6]{file_read_times.png}
  \caption{Left x-axis is the size of file and y-axis is average read time per block. Both are on log scale.}
  \label{fig:file_read_times}
\end{figure}


\subsubsection{Result Discussion}
We will now consider the results of the experiment as it currently stands. For
the sequential read, we see that there is a slight decreasing trend initially.
This was what we had anticipated in our estimations where we see that the
sequential reads are taking advantage locality and pre-fetching. However, we
don't see the same pattern in random reads because of the randomness with which
we area accessing the files' data. We see a slight increase in both sequential
and random file reading at around 2\textsuperscript{9.6}KB and an even more
drastic increase in time at 2\textsuperscript{19.6}KB\@. The larger increase is
due to files exceeding the size of the file cache, which we did not take into
account in our estimation. This increase is more drastic for random reads
because we are essentially making random reads from disk. This means that there
will be extra time needed to find the different blocks on disk. Compared to
sequential read which doesn't require jumping around to seek on the disk, there
is a larger increase in read time for random reads once the file exceeds cache
size.

Our methodology was successful overall as we can see the difference in time needed to make sequential and random read from different sized files. However, there is room for improvement. Had we ran the benchmarking for read times with a larger upper bound, we can expect to see when the read time for random reads will begin to plateau. This is the point in which the read time for a block will be dominated mostly by random read time from disk. Because we are measuring the average read over 100 iterations, we decided to end the testing prematurely due to time constraints.

Sequential access may not be sequential if given enough time or if trying to access a large file. Even though the system attempts to place related blocks in approximately the same area or sequentially, this effort may prove to be more difficult when there is not enough room to place blocks sequentially. In such a case, the blocks will be placed within some distance from its previous block. If the file is large enough, this non-sequential placement of blocks could result in high levels of fragmentation and be less sequential.

\subsection{Remote file read time}

\subsubsection{Experiment Methodology}
Once one have the random access and sequential access mechanism in the last subsection, we could
easily access the remote file through NFS protocol. Here we use two laptops, both of them are connected through an Eithernet cable to ensure better connection. The machine specification can been seen in Table~\ref{table:machine_description_remote}. Here we refer the remote machine to the server, and the machine our code runs on to the client.
On the client machine, we mount the server to using SSHFS and run the testing program.
On the server machine,  we enable the file sharing and add the permission for the client machine.
The testing schedule is exactly the same as what we mentioned in the previous section, except the target file is changed to /mnt/remote.

\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}

\subsection{Contention}

\subsubsection{Experiment Methodology}
\subsubsection{Prediction}
\subsubsection{Experiment Results}
\subsubsection{Result Discussion}

%============================= section ==========================%
\section{Time Consumption and Partition of work}
\subsubsection{CPU, Scheduling, and OS Services}
The first part of the project required around 20 hours of work to complete. This included creating the tools necessary to measure the operating system and writing the report.\\
The experiments were split accordingly:\\
Measurement Overhead - Boyuan\\
Procedure Call Overhead - Dexin\\
System Call Overhead - Boyuan/Qiheng\\
Task Creation Time - Dexin\\
Context Switch Time - Boyuan/Qiheng\\

The time we spent is:\\
Boyuan Qin: 16 + hours\\
Dexin Qi: 16+ hours\\
Qiheng Wang:16 + hours

\subsubsection{Memory}
The experiments were split accordingly:\\
RAM access time - Boyuan\\
RAM bandwidth - Dexin\\
Page fault service time - Qiheng\\

The time we spent is:\\
Boyuan Qin: 16 + hours\\
Dexin Qi: 16 + hours\\
Qiheng Wang:16 + hours

\subsubsection{Network}
The experiments were split accordingly:\\
Round trip time - Dexin\\
Peak bandwidth - Dexin\\
Connection overhead - Dexin\\

The time we spent is:\\
Dexin Qi: 30 + hours\\
\subsubsection{File System}
The experiments were split accordingly:\\
Size of file cache - Boyuan\\
File read time - Boyuan\\
Remote file read time - Qiheng \\
Contention - Qiheng\\

The time we spent is:\\
Boyuan Qin: 20 + hours\\
Qiheng Wang:

%============================= section ==========================%
\section{Complete Overview}

\begin{table}[!htbp]
  \caption{Overhead Operation Performance}
  \begin{center}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation              & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    \texttt{getticks}      & 8.3 ns                & 2.1 ns                & 10.4 ns               & 18.5 ns\\ \hline
    \texttt{elapsed}       & 10.4 ns               & 2.1 ns                & 12.5 ns               & 14.4 ns\\ \hline
    \texttt{for} loop      & 1.2 ns                & 0.8 ns                & 2.0 ns                & 2.8 ns\\ \hline
    System Call            & 4 ns                  & 3 ns                  & 7 ns                  & 96.94 ns\\ \hline
    Create Process         & 100 ns                & 100 ns                & 200 ns                & 11.75 ms\\ \hline
    Create Kernel Process  & 90 ns                 & 90 ns                 & 180 ns                & 28.53 ns\\ \hline
    Process Switch         & \textless50000 ns     & \textless20000 ns     & \textless70000 ns     & 0.1 mns\\ \hline
    Kernel Thread Switch   & \textless10000 ns     & \textless10000 ns     & \textless20000 ns     & 88.06 ns\\ \hline
  \end{tabular}
  \end{center}
  \label{table:overview_overhead}
\end{table}

\begin{table}[!htbp]
  \caption{Memory Operation Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation               & Hardware Est. & Software Est. & Prediction & Mean      \\ \hline
    L1 RAM Access           & 1.6 ns        & 0 ns          & 1.6 ns     & 3 ns      \\ \hline
    L2 RAM Access           & 3.2 ns        & 0 ns          & 3.2 ns     & 25 ns     \\ \hline
    Main Memory RAM Access  & 32 ns         & 0 ns          & 32 ns      & 275 ns    \\ \hline
    Read RAM Bandwidth      & 10.672GB/s    & -             & < 10GB/s   & 5480 MB/s \\ \hline
    Write RAM Bandwidth     & 10.672GB/s    & -             & < 10GB/s   & 2020 MB/s \\ \hline
    Page Fault Service Time & 10 ms         & 1 ms          & 11 ms      & 7.4 ms    \\ \hline
  \end{tabular}
  \label{table:overview_memory}
\end{table}

\begin{table}[!htbp]
  \caption{Network Performance}
  \begin{center}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation                              & Mean      \\ \hline
    TCP remote RTT                         & 0.270ms   \\ \hline
    TCP loopback RTT                       & 0.0462ms  \\ \hline
    ICMP remote ping latency               & 0.227ms   \\ \hline
    ICMP loopback ping latency             & 0.00918ms \\ \hline
    TCP remote peak bandwidth              & 11.2MB/s  \\ \hline
    TCP loopback peak bandwidth            & 191MB/s   \\ \hline
    TCP remote open connection overhead    & 0.252ms   \\ \hline
    TCP remote close connection overhead   & 0.00702ms \\ \hline
    TCP loopback open connection overhead  & 0.0486ms  \\ \hline
    TCP loopback close connection overhead & 0.0350ms  \\ \hline
  \end{tabular}
  \end{center}
  \label{table:overview_network}
\end{table}

\begin{table}[!htbp]
  \caption{File System Performance}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Operation               & Hardware Est.         & Software Est.         & Prediction            & Mean     \\ \hline
    File Cache Size         & \textless2GB          & n/a                   & \textless2GB          & 2\textsuperscript{19.6} to 2\textsuperscript{20.6}MB (about 1.13GB)\\ \hline
    Sequential Read Time    & 6600 ns               & 5 ns                  & 6605 ns               & 175125 ns\\ \hline
    Random Read Time        & 20600 ns              & 5 ns                  & 20605 ns              & 562180968 ns\\ \hline
    Remote Read Time        & ns                    & ns                    & ns                    & ns\\ \hline
    Remote Write Time       & ns                    & ns                    & ns                    & ns\\ \hline
    Contention              & ns                    & ns                    & ns                    & ns\\ \hline
  \end{tabular}
  \label{table:overview_file}
\end{table}


\newpage
\clearpage
%============================= section ==========================%
\section{Appendix}

\begin{table}[h]
  \caption{TCP remote round trip experiment original data}
  \begin{center}
    \resizebox{\columnwidth}{!}{
      \begin{tabular}{|>{\centering\arraybackslash\bfseries}m{1in}|l|l|l|l|}
        \hline
                                        & Experiment setting                     & mean       & standard deviation \\ \hline
        \multirow{4}{*}{Experiment 1}   & 100 tests each exp, repeat 100 times   & 0.44231897 & 0.242424045066     \\ \cline{2-4}
                                        & 1000 tests each exp, repeat 100 times  & 0.6776518  & 0.219758944135     \\ \cline{2-4}
                                        & 10000 tests each exp, repeat 10 times  & 0.5683592  & 0.201693566034     \\ \cline{2-4}
                                        & 100000 tests each exp, repeat 10 times & 0.3435085  & 0.0300421961656    \\ \cline{2-4}
        \hline
        \hline
        \multirow{3}{*}{Experiment 2}   & 1000 tests each exp, repeat 100 times  & 0.53642719 & 0.0546326544903    \\ \cline{2-4}
                                        & 10000 tests each exp, repeat 100 times & 0.51698054 & 0.0853942325217    \\ \cline{2-4}
                                        & 100000 tests each exp, repeat 10 times & 0.5349342  & 0.0235356311528    \\ \cline{2-4}
        \hline
        \hline
        \multirow{5}{*}{Experiment 3}   & 10 tests each exp, repeat 100 times    & 0.51347809 & 0.203160319043     \\ \cline{2-4}
                                        & 100 tests each exp, repeat 100 times   & 0.53865483 & 0.215805711464     \\ \cline{2-4}
                                        & 1000 tests each exp, repeat 100 times  & 0.53209326 & 0.142340688683     \\ \cline{2-4}
                                        & 1000 tests each exp, repeat 100 times  & 0.53520504 & 0.146021042698     \\ \cline{2-4}
                                        & 1000 tests each exp, repeat 100 times  & 0.53520504 & 0.146021042698     \\ \cline{2-4}
        \hline
      \end{tabular}
    }
  \end{center}
  \label{table:original_tcp_rrt}
\end{table}
\begin{table}[h]
  \caption{Machine Specifications for remove server}
  \begin{center}
    \resizebox{\columnwidth}{!}{
      \begin{tabular}{|>{\centering\arraybackslash\bfseries}m{1in}|l|l|}
        \hline
        \multirow{8}{*}{Processor}       & Model                                 & Intel\textregistered Core\texttrademark i5 CPU  M 460          \\ \cline{2-3}
                                         & Instruction Set                       & 64 bit                                                         \\ \cline{2-3}
                                         & Cycle Time                            & 0.395ns(2.53GHz frequency)                                     \\ \cline{2-3}
                                         & L1 data cache                         & 32KB per core, 8-way set associative, 64-byte line size        \\ \cline{2-3}
                                         & L1 instruction cache                  & 32KB per core, 8-way set associative, 64-byte line size        \\ \cline{2-3}
                                         & L2 data cache                         & 256KB, 8-way set associative, 64-byte line size                \\ \cline{2-3}
                                         & L3 data cache                         & 3072KB, 12-way set associative, 64-byte line size              \\ \cline{2-3}
                                         & FSB                                   & 1066 MHz                                                       \\
        \hline
        \multirow{11}{*}{Hard Drive}     & Model                                 & Seagate Momentus\textregistered  5400.6 SATA model ST9320325AS \\ \cline{2-3}
                                         & Capacity                              & 320GB                                                          \\ \cline{2-3}
                                         & Cache                                 & 8 Mbytes                                                       \\ \cline{2-3}
                                         & RPM                                   & 5400                                                           \\ \cline{2-3}
                                         & Physical heads                        & 4                                                              \\ \cline{2-3}
                                         & Discs                                 & 2                                                              \\ \cline{2-3}
                                         & Average Seek Read                     & 14ms typical                                                   \\ \cline{2-3}
                                         & Full Stroke Seek                      & 30ms                                                           \\ \cline{2-3}
                                         & Average Latency                       & 5.6ms                                                          \\ \cline{2-3}
                                         & Track to track seek time              & 1ms typical                                                    \\ \cline{2-3}
                                         & I/O data transfer rate                & 2400 Mbytes/s max                                               \\
        \hline
        \multirow{3}{*}{Memory}          & Capacity                              & Dual channel(symetric), each 2048 MBytes                       \\ \cline{2-3}
                                         & Frequency                             & DDR3 PC3-10700 1067 MHz                                         \\ \cline{2-3}
                                         & Width                                 & 64 bit per channel                                             \\
        \hline
        \multirow{2}{*}{Network Card}    & Model                                 & JMC250 PCI Express Gigabit Ethernet Controller                 \\ \cline{2-3}
                                         & Data Transfer Rate                    & 10/100/1000Mbps                                                \\
        \hline
        \multicolumn{1}{|>{\bfseries}c|}{\multirow{2}{*}{OS}} & \multirow{2}{*}{ Linux Distribution } & Linux Mint 15 Olivia                        \\
                                                              & \multicolumn{1}{c|}{}                 & GNU/Linux 3.8.0-35-generic x86\_64           \\
  % here if use |c| instead of c|, will generate an extra vertical line
        \hline
      \end{tabular}
    } % end of resizebox
  \end{center}
  \label{table:machine_description_remote}
\end{table}
%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\newpage
\clearpage
%============================= section ==========================%
\begin{thebibliography}{1}

  \bibitem{FFTW} Cycle Counters. Available at {\tt http://www.fftw.org/download.html}.
  \bibitem{}http://en.wikipedia.org/wiki/Penryn\_(microprocessor)
  \bibitem{}http://tuxthink.blogspot.com/2011/02/kernel-thread-creation-1.html
  \bibitem{}http://jahanzebnotes.blogspot.com/2013/02/turn-off-cpu-throttling-ubuntu.html
  \bibitem{}http://www.seagate.com/staticfiles/support/disc/manuals/notebook/momentus/5400.6\%20(Wyatt)/100528359e.pdf
  \bibitem{}http://www.cpuid.com/softwares/cpu-z.html
  \bibitem{}Larry McVoy and Carl Staelin. 1996. lmbench: portable tools for performance analysis. In Proceedings of the 1996 annual conference on USENIX Annual Technical Conference (ATEC '96). USENIX Association, Berkeley, CA, USA, 23-23.
  \bibitem{}http://codearcana.com/posts/2013/05/18/achieving-maximum-memory-bandwidth.html
  \bibitem{}http://www.nasm.us/doc/nasmdoc6.html
  \bibitem{}http://zsmith.co/bandwidth.html
  \bibitem{}http://www.sisoftware.net/?d=qa\&f=ben\_mem\_latency
  \bibitem{}http://www.bitmover.com/lmbench/
  \bibitem{}http://www.tcpipguide.com/free/t\_TCPConnectionTermination-2.htm
  \bibitem{}http://www.tcpipguide.com/free/t\_TCPConnectionEstablishmentProcessTheThreeWayHandsh-3.htm
  \bibitem{}http://www.tcpipguide.com/free/t\_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm
  \bibitem{GNU}http://www.gnu.org/software/libc/manual/html\_node/Closing-a-Socket.html

\end{thebibliography}

\end{document}
